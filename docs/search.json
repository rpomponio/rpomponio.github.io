[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Work In Progress",
    "section": "",
    "text": "The following work has been submitted for consideration at the 2024 SABR Analytics Conference in Phoenix, Arizona: conference page.\n\n\nRecently a number of research papers have emerged examining the effects of starting pitcher replacements in Major League Baseball (MLB).1 2 3 This past year, the time through the order penalty (TTOP) was analyzed in a Bayesian framework and results were published in the Journal of Quantiative Analysis in Sports.4 There remain many questions about starting pitcher strategies, particularly in the postseason when managers’ decisions are hyper-scrutinized.5 Despite a trend towards earlier starting pitcher replacements in both regular season6 7 and postseason8 9 play, I am not aware of any analysis that has considered the replacement of the starting pitcher as a time-to-event phenomenon. The field of biostatistics has benefited greatly from the ability to model time-to-event outcomes, such as all-cause mortality, within a regression framework outlined by the late David Cox.10 Cox’s proportional hazards model is convenient for several reasons. It permits the formal modeling of a relationship between time and the event of interest; it incorporates covariates that increase or reduce the risk of the event; and, with modern software one can easily estimate the probability of an event occurring within a pre-specified time window. All of these characteristics are useful in the context of analyzing starting pitcher replacements. I will make the case that Cox’s model is more suitable for modeling pitcher replacements than alternative approaches using classification or supervised machine learning techniques.\n\n\n\nIn game seven of the 2003 ALCS, the Red Sox held a two-run lead over the Yankees in the eighth inning when manager Grady Little visited the mound.11 Starting pitcher Pedro Martínez was still in the game, with Hideki Matsui at the plate. Little decided to leave Martínez in for both Matsui and Jorge Posada, resulting in the loss of the lead and ultimately the game in 11 innings. At the time of Little’s mound visit, how likely was Martínez’s replacement? The model that is developed below suggests that Martínez would be pulled in 60.4% of games with similar circumstances.\n\n\n\nOther sabermetrics work has identified important in-game factors that predict a starter’s replacement, such as the strike count, pitch count, number of outs, total batters faced, and current home runs conceded. However, pitching replacements have not been modeled as time-to-event outcomes. Instead, they have been modeled as binary outcomes. The advantage of timing models is that the cumulative effect of time is inherent; certain factors can expedite, or delay, the timing of the event. This process is also more realistic than a binary outcomes process, which considers each event independently.\nThe first question in this analysis is, what is the unit of time? For studying outcomes in medicine, time is often measured in months or years. However, in a baseball game, no more than a few hours go by, and typically the starting pitcher is replaced mid-way through the game. I chose to use the number of batters faced as the unit of time in this analysis. While this is not a measure of clock time, it is a measure of the number of opponents that the pitcher has faced. The higher the number of opponents, the more fatigued the pitcher will become. Alternatively one could consider the number of pitches thrown as the unit of time. I don’t see any issue with this, though pitcher replacements often happen between batter plate appearanced, and pitch-by-pitch data is much more granular, potentially complicating the analysis\nFor data, I used all postseason games from 2000-2019 (source: RetroSheet.org). There were several reasons for choosing these seasons and not including more recent seasons. First, the obvious complications of the COVID-19 pandemic make working with the 2020 postseason challenging. Secondly, a number of recent rule changes have been implemented which might influence the strategy around starting pitcher replacement, for example the pitch clock. Therefore I consider the 20-year window between 2000 and 2019 to be a relatively stable period of postseason play. Future work will have to revisit the same questions explored here and see if the underlying conclusions hold up in the modern era.\nI used Cox’s proportional hazards modeling approach and controlled for in-game strikeout rate, pitching volume (pitches thrown per batter faced), runners in scoring position, and home runs conceded, as well as differences across season eras and leagues. All of the covariates included in the model were computed sequentially for each batter faced. In other words, I did not use data from later in the game to inform a covariate value earlier in the game. I grouped seasons into five-year eras (e.g., 2000-2004), then analyzed the time-to-replacement for all starting pitchers in the dataset.\n\n\n\nI found that pitchers in the 2015-2019 era were far more likely to be replaced earlier in the game, versus their counterparts in previous seasons. Below is an illustration of the ‘survival probability’, or the probability of remaining in the game for a starting pitcher from four different eras. It is clear that in the most-recent era (the purple line), starting pitchers had lower probabilities of remaining in the game at each number of batters faced (TBF). The p-value printed in the lower left is a result of comparing the four curves and assessing whether one or more curves differs from the rest. It is apparent that the difference in ‘survival’ in the 2015-2019 era, versus previous eras, was statistically significant.\n\n\n\nKaplan Meier curves illustrating probabilities of remaining in-game for starting pitchers in postseason games 2000-2019. The result of the log-rank test suggests a difference across season-eras.\n\n\nBelow are the model estimates for the effect of each covariate. For example, the Cox adjusted Hazard Ratio of Season era 2015-2019 (versus 2000-2004) was 2.69 (95% CI: 2.29 – 3.16). This means that starting pitchers in the most-recent era were 169% more likely to be replaced against any given batter, versus the 2000-2004 era. The relatively narrow confidence interval tells us that the estimated hazard ratio is reliable, i.e., if we were to replay history we would likely obtain a similar estimate.\n\n\n\nEstimated hazard ratios for the ‘replacement factors’ of starting pitchers. The replacement factors are thought to influence the timing of pitcher replacements in the postseason. A hazard ratio greater than one indicates a higher likelihood of pitcher replacement.\n\n\nTo illustrate the magnitude of the season-era effect, consider a hypothetical pitcher playing in both the 2000-2004 and the 2000-2019 eras. Assuming typical starter performance (3.8 pitches per batter faced, strikeout rate of 20%), this pitcher has a 70.3% chance of pitching a third time through the order in 2000-2004 but only a 38.8% chance of doing so in 2015-2019. With a runner in scoring position, these probabilities decline to 60.5% and 25.8%, respectively (with one home run conceded, the probabilities are similar). The probabilities below reflect the two scenarios, with the baseline scenario being no runners in scoring position (RISP) and no home runs conceded.\n\n\n\nPredicted probabilities of pitching a third time through the order for two hypothetical pitchers. The only difference between the pitchers is the era in which they play. The RISP scenario means at least one runner is in scoring position.\n\n\n\n\n\nThis work quantifies trends in starting pitcher replacements that have been formerly described, but not modeled in a time-to-event fashion. We can conclude that the strategy around starting pitcher replacement has been changing over the past 20 years of postseason play, since the effect of season-era is so significant. On the contrary, the use of the DH did not substantially alter the chance of pulling the starter when comparing managerial decisions across leagues.\nAside from the apparent trend of earlier staring pitcher replacements, there are several factors that indicate a meaningful impact on managerial strategy. For example, pitchers throwing more than 3.8 pitches per batter faced were more likely to be pulled (each additional pitch thrown per batter was associated with 127% higher chance of replacement). Similarly, a pitcher throwing to an opposite-handed batter was more likely to be pulled. Lastly, pitchers generating more than a 20% in-game strikeout rate were less likely to be pulled (each additional 10 percentage-point increase in K rate was associated with 28% lower chance of replacement).\nThe model has drawbacks, in that it does not capture head-to-head matchup history or differing styles of management. Those factors can be included in future work. First, for head-to-head matchup history, it would likely encompass an estimate of the on-base percentage of the batter against the specific pitcher in-question, observed during in the regular season of that same year. Ideally, this metric would be robust to small sample sizes, which are common when two opponents do not face each other very often in a season. The best framework for this would be an empirical Bayes estimate of head-to-head OBP.12\nSecondly, for managerial styles, it would make sense to specify a random effect to account for correlated outcomes from decisions made by the same manager. In other words, some managers would be likely to pull the starter early; others may have a tendency to leave him in. This involves a more advanced modeling approach but it is still possible within the broader Cox proportional hazards framework. An analysis that accounts for managerial styles would offer empirical evidence of any trends among certain managers that have been observed by the sports commentary community.\n\n\n\nPostseason play-by-play data was downloaded from RetroSheet’s postseason event files. In total, 611 games were included. Postseason rosters indicated the batting and throwing handedness of individual players. The use of a designated hitter was determined by the starting roster for each game.\nThe play-by-play data contain one row per ‘play’, where a play constitutes either a base-running event or a ball put into play by the batter (or both). To make data wrangling more manageable, all offesnive plays after the replacement of the opponent starting pitcher were discarded; this was done separately for each team. For the start of each batter’s plate appearance, several key factors were determined:\n\nBatter handedness: same as pitchers (yes/no)\nRunners in scoring position (yes/no)\nNumber of pitches thrown by stater (e.g., 0, 1, 2, …)\nHomeruns conceded by starter in the game\nPitcher strikeout rate in the game (e.g., 20%)\n\nImportantly, the above factors were determined without knowledge of the outcome of the batter’s plate appearance. Thus, the information used in modeling simulates all information available to a manager in deciding whether to replace a pitcher at the start of an at-bat.\nThe factors used to predict the replacement of a starting pitcher were called ‘replacement factors.’ In this analysis, only a limited set of replacement factors were considered. Note the average pitching volume was 3.8 pitches per batter faced; the average strikeout rate was 20%. To aid interpretability, the replacement factors for pitching volume and strikeout rate were centered at zero, therefore a positive value for either factor represents above-average.Above-average pitching volume is generally considered worse performance, as the pitcher must throw more times to the opponent and is expected to fatigues earlier. In contrast, above-average strikeout rate indicates elevated performance.\nOf the 1,218 starting pitching performances in the dataset, only 33 (2.7%) included complete game performances; complete games are very rare but not unheard of in the postseason.13 Fortunately the Cox proportional hazards framework does not require discarding complete games, even though the starting pitcher was never replaced. Instead, this is considered a censored observation, where we know that the pitcher remained in the game for its entirety. The underlying assumption is that the pitcher would have been replaced, for example if the game extended into extra innings. This is not an unreasonable assumption as even the best-throwing pitchers will eventually fatigue and need relief, if for no other reason than injury management.\nLinks: GitHub | Google Scholar"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Graduated May 2023, Master of Science in Biostatistics: University of Colorado (Denver, CO).\nBachelor of Science in Economics: University of Pennsylvania - Wharton (W’ 2018, Magna Cum Laude).\nExpertise in longitudinal data analysis and advanced statistical computing using R, SAS, and python.\nPast experience includes work in sports analytics, marketing, neuroimaging, and critical care medicine.\nOriginally from Philadelphia, PA.\nCurrently lives in Shadyside (Pittsburgh, PA).\nTo contact me, please reach me at: raymond.pomponio (at) outlook.com.\nFor freelance data science work, you can hire me on Upwork.\nLinks: GitHub | Google Scholar"
  },
  {
    "objectID": "index.html#executive-summary",
    "href": "index.html#executive-summary",
    "title": "Ray Pomponio",
    "section": "Executive Summary",
    "text": "Executive Summary\nIn game seven of the 2003 ALCS, the Red Sox held a two-run lead in the eighth inning when manager Grady Little visited the mound1. Starting pitcher Pedro Martínez was still in the game, with Hideki Matsui at the plate. Little decided to leave Martínez in for both Matsui and Jorge Posada, resulting in the loss of the lead and ultimately the game in 11 innings. At the time of Little’s mound visit, how likely was Martínez’s replacement? The following model suggests that Martínez would be pulled in 60.4% of games with similar circumstances. In postseason play, the timing of pulling a starter is the most crucial in-game decision faced by managers2. Recent sabermetrics work has identified a trend towards earlier starting pitcher replacements in regular season3 4 5 and postseason play6 7. Previous work on the topic modeled the in-game performance of starting pitchers, using short-term forecasts to evaluate managerial decisions, and found that managers were too often leaving starters in the game longer than warranted by analytics8. More recently, an evaluation of managers’ decisions found no biases in pulling pitchers too early or too late when considering effects on win probability9. Other work has identified important in-game factors that predict a starter’s replacement, such as the strike count, pitch count, number of outs, total batters faced, and current home runs conceded10. However, pitching replacements have not been modeled as time-to-event outcomes; instead, they have often been modeled as binary outcomes, as in machine learning11 12. The advantage of timing models is that the cumulative effect of time is inherent; certain factors can expedite, or delay, the timing of the event. This process is more realistic than a binary outcomes model, which considers each event independently. I modeled the time-to-replacement for starting pitchers with time as the number of batters faced, using all postseason games from 2000-2019 (source: RetroSheet.org). I used a flexible modeling approach that controls for in-game strikeout rate, pitching volume, runners in scoring position, and home runs conceded, as well as differences across season eras and leagues. I found that postseason starters in the 2015-2019 era were far more likely to be replaced while facing any given batter, compared to starters in the 2000-2004 era (Cox adjusted Hazard Ratio13: 2.69, 95% CI: 2.29 – 3.16). To illustrate the magnitude of this effect, consider a hypothetical pitcher playing in both eras. Assuming typical starter performance (3.8 pitches per batter faced, strikeout rate of 20%), this pitcher has a 70.3% chance of pitching a third time through the order in 2000-2004 but only a 38.8% chance of doing so in 2015-2019. With a runner in scoring position, these probabilities decline to 60.5% and 25.8%, respectively (with one home run conceded, the probabilities are similar). This work quantifies trends in starting pitcher replacements that have been formerly described, but not modeled in a time-to-event fashion. The model has drawbacks, in that it does not capture head-to-head matchup history or differing styles of management, but those factors will be included in future work.\nResearch focus: pitcher replacements, bullpen strategy, managerial decision-making."
  },
  {
    "objectID": "index.html#time-to-replacement-in-postseason",
    "href": "index.html#time-to-replacement-in-postseason",
    "title": "Ray Pomponio",
    "section": "",
    "text": "Preliminary evidence suggests a trend towards earlier replacement of the starting pitcher in postseason games.\n\n\n\nKaplan Meier curves illustrating time-to-replacement for starting pitchers in all postseason games 2000-2019. Result of the log-rank test suggests a difference in survival across season-eras.\n\n\n\n[1] D. Finigan, B. M. Mills and D. F. Stone, “Pulling starters,” Journal of Behavioral and Experimental Economics, vol. 89, 2020. [2] R. Houston, “Call to the Bullpen: More Often and More Effective,” Samford University, 20 April 2018. [Online]. Available: https://www.samford.edu/sports-analytics. [Accessed November 2023]. [3] M. Orwin and C. Tien, “Call to the Bullpen: The Increase in MLB Reliever Usage,” Bruin Sports Analytics, 31 March 2022. [Online]. Available: https://www.bruinsportsanalytics.com/. [Accessed November 2023]. [4] J. Jaffe, “The Incredible Shrinking Postseason Starter,” FanGraphs, 18 October 2021. [Online]. Available: https://blogs.fangraphs.com/. [Accessed November 2023]. [5] J. Jaffe, “Postseason Starting Pitching Ain’t What It Used to Be, After All,” FanGraphs, 27 October 2023. [Online]. Available: https://blogs.fangraphs.com/. [Accessed November 2023]. [6] G. Ganeshapillai and J. Guttag, “A Data-driven Method for In-game Decision Making in MLB,” MIT Sloan Sports Analytics Conference, vol. 8, 2014. [7] M. Woodham, J. Hawkins, A. Singh and S. Chakraborty, “When to Pull Starting Pitchers in Major League Baseball? A Data Mining Approach,” IEEE International Conference On Machine Learning And Applications, vol. 18, pp. 426-431, 2019. [8] P. Schober and T. R. Vetter, “Survival Analysis and Interpretation of Time-to-Event Data: The Tortoise and the Hare,” Anesthesia and analgesia, vol. 127, no. 3, pp. 792-798, 2018."
  },
  {
    "objectID": "index.html#results-overview",
    "href": "index.html#results-overview",
    "title": "Ray Pomponio",
    "section": "",
    "text": "I found that pitchers in the 2015-2019 era were far more likely to be replaced earlier in the game, versus their counterparts in previous seasons.\n\n\n\nKaplan Meier curves illustrating probabilities of remaining in-game for starting pitchers in postseason games 2000-2019. The result of the log-rank test suggests a difference across season-eras.\n\n\nBelow are the model estimates for the effect of each covariate. The Cox adjusted Hazard Ratio of Season era 2015-2019 (versus 2000-2004) was 2.69 (95% CI: 2.29 – 3.16).\n\n\n\nEstimated hazard ratios for the ‘replacement factors’ of starting pitchers. The replacement factors are thought to influence the timing of pitcher replacements in the postseason. A hazard ratio greater than one indicates a higher likelihood of pitcher replacement.\n\n\nTo illustrate the magnitude of the season effect, consider a hypothetical pitcher playing in both eras. Assuming typical starter performance (3.8 pitches per batter faced, strikeout rate of 20%), this pitcher has a 70.3% chance of pitching a third time through the order in 2000-2004 but only a 38.8% chance of doing so in 2015-2019. With a runner in scoring position, these probabilities decline to 60.5% and 25.8%, respectively (with one home run conceded, the probabilities are similar). The probabilities below reflect the two scenarios, with the baseline scenario being no runners in scoring position (RISP) and no home runs conceded.\n\n\n\nPredicted probabilities of pitching a third time through the order for two hypothetical pitchers. The only difference between the pitchers is the era in which they play. The RISP scenario means at least one runner is in scoring position."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Ray Pomponio",
    "section": "",
    "text": "[1] D. Finigan, B. M. Mills and D. F. Stone, “Pulling starters,” Journal of Behavioral and Experimental Economics, vol. 89, 2020.\n[2] R. Houston, “Call to the Bullpen: More Often and More Effective,” Samford University, 20 April 2018. [Online]. Available: https://www.samford.edu/sports-analytics. [Accessed November 2023].\n[3] M. Orwin and C. Tien, “Call to the Bullpen: The Increase in MLB Reliever Usage,” Bruin Sports Analytics, 31 March 2022. [Online]. Available: https://www.bruinsportsanalytics.com/. [Accessed November 2023].\n[4] J. Jaffe, “The Incredible Shrinking Postseason Starter,” FanGraphs, 18 October 2021. [Online]. Available: https://blogs.fangraphs.com/. [Accessed November 2023].\n[5] J. Jaffe, “Postseason Starting Pitching Ain’t What It Used to Be, After All,” FanGraphs, 27 October 2023. [Online]. Available: https://blogs.fangraphs.com/. [Accessed November 2023].\n[6] G. Ganeshapillai and J. Guttag, “A Data-driven Method for In-game Decision Making in MLB,” MIT Sloan Sports Analytics Conference, vol. 8, 2014.\n[7] M. Woodham, J. Hawkins, A. Singh and S. Chakraborty, “When to Pull Starting Pitchers in Major League Baseball? A Data Mining Approach,” IEEE International Conference On Machine Learning And Applications, vol. 18, pp. 426-431, 2019.\n[8] P. Schober and T. R. Vetter, “Survival Analysis and Interpretation of Time-to-Event Data: The Tortoise and the Hare,” Anesthesia and analgesia, vol. 127, no. 3, pp. 792-798, 2018."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Work In Progress",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nG. Ganeshapillai and J. Guttag, “A Data-driven Method for In-game Decision Making in MLB,” MIT Sloan Sports Analytics Conference, vol. 8, 2014.↩︎\nM. Woodham, J. Hawkins, A. Singh and S. Chakraborty, “When to Pull Starting Pitchers in Major League Baseball? A Data Mining Approach,” IEEE International Conference On Machine Learning And Applications, vol. 18, pp. 426-431, 2019.↩︎\nD. Finigan, B. M. Mills and D. F. Stone, “Pulling starters,” Journal of Behavioral and Experimental Economics, vol. 89, 2020.↩︎\nBrill, R., Deshpande, S. & Wyner, A. “A Bayesian analysis of the time through the order penalty in baseball.” Journal of Quantitative Analysis in Sports, 19(4), 245-262, 2023.↩︎\nJ. Stark, “The book on hooks,” ESPN.com Baseball, 10 May 2004. [Online]. Available: https://www.espn.com/. [Accessed November 2023].↩︎\nR. Houston, “Call to the Bullpen: More Often and More Effective,” Samford University, 20 April 2018. [Online]. Available: https://www.samford.edu/sports-analytics. [Accessed November 2023].↩︎\nM. Orwin and C. Tien, “Call to the Bullpen: The Increase in MLB Reliever Usage,” Bruin Sports Analytics, 31 March 2022. [Online]. Available: https://www.bruinsportsanalytics.com/. [Accessed November 2023].↩︎\nJ. Jaffe, “Postseason Starting Pitching Ain’t What It Used to Be, After All,” FanGraphs, 27 October 2023. [Online]. Available: https://blogs.fangraphs.com/. [Accessed November 2023].↩︎\nJ. Jaffe, “The Incredible Shrinking Postseason Starter,” FanGraphs, 18 October 2021. [Online]. Available: https://blogs.fangraphs.com/. [Accessed November 2023].↩︎\nJ. D. Kalbfleisch and D. E. Schaubel, “Fifty Years of the Cox Model,” Annual Review of Statistics and Its Application, vol. 10, pp. 1-23, 2023.↩︎\nJ. D. Kalbfleisch and D. E. Schaubel, “Fifty Years of the Cox Model,” Annual Review of Statistics and Its Application, vol. 10, pp. 1-23, 2023.↩︎\nA. Landgraf, “Empirical Bayes Estimation of On Base Percentage,” R-bloggers, 30 December 2010 [Online]. Available: https://www.r-bloggers.com/2010/12/empirical-bayes-estimation-of-on-base-percentage/. [Accessed December 2023].↩︎\nL. Shenk, “Looking back at Doc’s postseason no-no,” MLB.com, 13 October 2022 [Online]. Available: https://www.mlb.com/news/roy-halladay-s-playoff-no-hitter-remembered. [Accessed December 2023].↩︎"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Ray Pomponio",
    "section": "",
    "text": "Recently a number of research papers have emerged examining the effects of starting pitcher replacements in Major League Baseball (MLB).1 2 This past year, the time through the order penalty (TTOP) was analyzed in a Bayesian framework and results were published in the Journal of Quantiative Analysis in Sports.3 There remain many questions about starting pitcher strategies, particularly in the postseason when managers’ decisions are hyper-scrutinized.4 Despite a trend towards earlier starting pitcher replacements in both regular season5 6 and postseason7 8 play, I am not aware of any analysis that has considered the replacement of the starting pitcher as a time-to-event phenomenon. The field of biostatistics has benefited greatly from the ability to model time-to-event outcomes, such as all-cause mortality, within a regression framework outlined by the late David Cox.9 Cox’s proportional hazards model is convenient for several reasons. It permits the formal modeling of a relationship between time and the event of interest; it incorporates covariates that increase or reduce the risk of the event; and, with modern software one can easily estimate the probability of an event occurring within a pre-specified time window. All of these characteristics are useful in the context of analyzing starting pitcher replacements. I will make the case that Cox’s model is more suitable for modeling pitcher replacements than alternative approaches using classification or supervised machine learning techniques."
  },
  {
    "objectID": "index.html#motivating-example",
    "href": "index.html#motivating-example",
    "title": "Ray Pomponio",
    "section": "",
    "text": "In game seven of the 2003 ALCS, the Red Sox held a two-run lead in the eighth inning when manager Grady Little visited the mound.10 Starting pitcher Pedro Martínez was still in the game, with Hideki Matsui at the plate. Little decided to leave Martínez in for both Matsui and Jorge Posada, resulting in the loss of the lead and ultimately the game in 11 innings. At the time of Little’s mound visit, how likely was Martínez’s replacement? The model that is developed below suggests that Martínez would be pulled in 60.4% of games with similar circumstances."
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "Ray Pomponio",
    "section": "",
    "text": "Other work has identified important in-game factors that predict a starter’s replacement, such as the strike count, pitch count, number of outs, total batters faced, and current home runs conceded. However, pitching replacements have not been modeled as time-to-event outcomes; instead, they have often been modeled as binary outcomes. The advantage of timing models is that the cumulative effect of time is inherent; certain factors can expedite, or delay, the timing of the event. This process is more realistic than a binary outcomes model, which considers each event independently.\nI modeled the time-to-replacement for starting pitchers with time in terms of total batters faced (TBF), using all postseason games from 2000-2019 (source: RetroSheet.org). I used Cox’s proportional hazards modeling approach and controlled for in-game strikeout rate, pitching volume (pitches thrown per batter faced), runners in scoring position, and home runs conceded, as well as differences across season eras and leagues.\nAll of the factors included as model covariates were computed sequentially for each batter faced, i.e., I did not use data from later in the game to inform a prediction earlier in the game. I grouped seasons into five-year eras, then analyzed the time-to-replacement for all starting pitchers in the postseason."
  },
  {
    "objectID": "index.html#conclusions",
    "href": "index.html#conclusions",
    "title": "Ray Pomponio",
    "section": "",
    "text": "This work quantifies trends in starting pitcher replacements that have been formerly described, but not modeled in a time-to-event fashion. The model has drawbacks, in that it does not capture head-to-head matchup history or differing styles of management, but those factors will be included in future work."
  },
  {
    "objectID": "index.html#sabermetric-analysis-of-time-to-replacement-for-starting-pitchers",
    "href": "index.html#sabermetric-analysis-of-time-to-replacement-for-starting-pitchers",
    "title": "Ray Pomponio",
    "section": "",
    "text": "The following work has been submitted for consideration at the 2024 SABR Analytics Conference in Phoenix, Arizona: conference page."
  },
  {
    "objectID": "index.html#modeling-the-timing-of-starting-pitcher-replacements-in-postseason-games",
    "href": "index.html#modeling-the-timing-of-starting-pitcher-replacements-in-postseason-games",
    "title": "Ray Pomponio",
    "section": "",
    "text": "The following work has been submitted for consideration at the 2024 SABR Analytics Conference in Phoenix, Arizona: conference page.\n\n\nRecently a number of research papers have emerged examining the effects of starting pitcher replacements in Major League Baseball (MLB).1 2 3 This past year, the time through the order penalty (TTOP) was analyzed in a Bayesian framework and results were published in the Journal of Quantiative Analysis in Sports.4 There remain many questions about starting pitcher strategies, particularly in the postseason when managers’ decisions are hyper-scrutinized.5 Despite a trend towards earlier starting pitcher replacements in both regular season6 7 and postseason8 9 play, I am not aware of any analysis that has considered the replacement of the starting pitcher as a time-to-event phenomenon. The field of biostatistics has benefited greatly from the ability to model time-to-event outcomes, such as all-cause mortality, within a regression framework outlined by the late David Cox.10 Cox’s proportional hazards model is convenient for several reasons. It permits the formal modeling of a relationship between time and the event of interest; it incorporates covariates that increase or reduce the risk of the event; and, with modern software one can easily estimate the probability of an event occurring within a pre-specified time window. All of these characteristics are useful in the context of analyzing starting pitcher replacements. I will make the case that Cox’s model is more suitable for modeling pitcher replacements than alternative approaches using classification or supervised machine learning techniques.\n\n\n\nIn game seven of the 2003 ALCS, the Red Sox held a two-run lead in the eighth inning when manager Grady Little visited the mound.11 Starting pitcher Pedro Martínez was still in the game, with Hideki Matsui at the plate. Little decided to leave Martínez in for both Matsui and Jorge Posada, resulting in the loss of the lead and ultimately the game in 11 innings. At the time of Little’s mound visit, how likely was Martínez’s replacement? The model that is developed below suggests that Martínez would be pulled in 60.4% of games with similar circumstances.\n\n\n\nOther work has identified important in-game factors that predict a starter’s replacement, such as the strike count, pitch count, number of outs, total batters faced, and current home runs conceded. However, pitching replacements have not been modeled as time-to-event outcomes; instead, they have often been modeled as binary outcomes. The advantage of timing models is that the cumulative effect of time is inherent; certain factors can expedite, or delay, the timing of the event. This process is more realistic than a binary outcomes model, which considers each event independently.\nI modeled the time-to-replacement for starting pitchers with time in terms of total batters faced (TBF), using all postseason games from 2000-2019 (source: RetroSheet.org). I used Cox’s proportional hazards modeling approach and controlled for in-game strikeout rate, pitching volume (pitches thrown per batter faced), runners in scoring position, and home runs conceded, as well as differences across season eras and leagues.\nAll of the factors included as model covariates were computed sequentially for each batter faced, i.e., I did not use data from later in the game to inform a prediction earlier in the game. I grouped seasons into five-year eras, then analyzed the time-to-replacement for all starting pitchers in the postseason.\n\n\n\nI found that pitchers in the 2015-2019 era were far more likely to be replaced earlier in the game, versus their counterparts in previous seasons.\n\n\n\nKaplan Meier curves illustrating probabilities of remaining in-game for starting pitchers in postseason games 2000-2019. The result of the log-rank test suggests a difference across season-eras.\n\n\nBelow are the model estimates for the effect of each covariate. The Cox adjusted Hazard Ratio of Season era 2015-2019 (versus 2000-2004) was 2.69 (95% CI: 2.29 – 3.16).\n\n\n\nEstimated hazard ratios for the ‘replacement factors’ of starting pitchers. The replacement factors are thought to influence the timing of pitcher replacements in the postseason. A hazard ratio greater than one indicates a higher likelihood of pitcher replacement.\n\n\nTo illustrate the magnitude of the season effect, consider a hypothetical pitcher playing in both eras. Assuming typical starter performance (3.8 pitches per batter faced, strikeout rate of 20%), this pitcher has a 70.3% chance of pitching a third time through the order in 2000-2004 but only a 38.8% chance of doing so in 2015-2019. With a runner in scoring position, these probabilities decline to 60.5% and 25.8%, respectively (with one home run conceded, the probabilities are similar). The probabilities below reflect the two scenarios, with the baseline scenario being no runners in scoring position (RISP) and no home runs conceded.\n\n\n\nPredicted probabilities of pitching a third time through the order for two hypothetical pitchers. The only difference between the pitchers is the era in which they play. The RISP scenario means at least one runner is in scoring position.\n\n\n\n\n\nThis work quantifies trends in starting pitcher replacements that have been formerly described, but not modeled in a time-to-event fashion. The model has drawbacks, in that it does not capture head-to-head matchup history or differing styles of management, but those factors will be included in future work."
  },
  {
    "objectID": "index.html#modeling-the-timing-of-starting-pitcher-replacements-in-postseason-games-2023",
    "href": "index.html#modeling-the-timing-of-starting-pitcher-replacements-in-postseason-games-2023",
    "title": "Work In Progress",
    "section": "",
    "text": "The following work has been submitted for consideration at the 2024 SABR Analytics Conference in Phoenix, Arizona: conference page.\n\n\nRecently a number of research papers have emerged examining the effects of starting pitcher replacements in Major League Baseball (MLB).1 2 3 This past year, the time through the order penalty (TTOP) was analyzed in a Bayesian framework and results were published in the Journal of Quantiative Analysis in Sports.4 There remain many questions about starting pitcher strategies, particularly in the postseason when managers’ decisions are hyper-scrutinized.5 Despite a trend towards earlier starting pitcher replacements in both regular season6 7 and postseason8 9 play, I am not aware of any analysis that has considered the replacement of the starting pitcher as a time-to-event phenomenon. The field of biostatistics has benefited greatly from the ability to model time-to-event outcomes, such as all-cause mortality, within a regression framework outlined by the late David Cox.10 Cox’s proportional hazards model is convenient for several reasons. It permits the formal modeling of a relationship between time and the event of interest; it incorporates covariates that increase or reduce the risk of the event; and, with modern software one can easily estimate the probability of an event occurring within a pre-specified time window. All of these characteristics are useful in the context of analyzing starting pitcher replacements. I will make the case that Cox’s model is more suitable for modeling pitcher replacements than alternative approaches using classification or supervised machine learning techniques.\n\n\n\nIn game seven of the 2003 ALCS, the Red Sox held a two-run lead over the Yankees in the eighth inning when manager Grady Little visited the mound.11 Starting pitcher Pedro Martínez was still in the game, with Hideki Matsui at the plate. Little decided to leave Martínez in for both Matsui and Jorge Posada, resulting in the loss of the lead and ultimately the game in 11 innings. At the time of Little’s mound visit, how likely was Martínez’s replacement? The model that is developed below suggests that Martínez would be pulled in 60.4% of games with similar circumstances.\n\n\n\nOther sabermetrics work has identified important in-game factors that predict a starter’s replacement, such as the strike count, pitch count, number of outs, total batters faced, and current home runs conceded. However, pitching replacements have not been modeled as time-to-event outcomes. Instead, they have been modeled as binary outcomes. The advantage of timing models is that the cumulative effect of time is inherent; certain factors can expedite, or delay, the timing of the event. This process is also more realistic than a binary outcomes process, which considers each event independently.\nThe first question in this analysis is, what is the unit of time? For studying outcomes in medicine, time is often measured in months or years. However, in a baseball game, no more than a few hours go by, and typically the starting pitcher is replaced mid-way through the game. I chose to use the number of batters faced as the unit of time in this analysis. While this is not a measure of clock time, it is a measure of the number of opponents that the pitcher has faced. The higher the number of opponents, the more fatigued the pitcher will become. Alternatively one could consider the number of pitches thrown as the unit of time. I don’t see any issue with this, though pitcher replacements often happen between batter plate appearanced, and pitch-by-pitch data is much more granular, potentially complicating the analysis\nFor data, I used all postseason games from 2000-2019 (source: RetroSheet.org). There were several reasons for choosing these seasons and not including more recent seasons. First, the obvious complications of the COVID-19 pandemic make working with the 2020 postseason challenging. Secondly, a number of recent rule changes have been implemented which might influence the strategy around starting pitcher replacement, for example the pitch clock. Therefore I consider the 20-year window between 2000 and 2019 to be a relatively stable period of postseason play. Future work will have to revisit the same questions explored here and see if the underlying conclusions hold up in the modern era.\nI used Cox’s proportional hazards modeling approach and controlled for in-game strikeout rate, pitching volume (pitches thrown per batter faced), runners in scoring position, and home runs conceded, as well as differences across season eras and leagues. All of the covariates included in the model were computed sequentially for each batter faced. In other words, I did not use data from later in the game to inform a covariate value earlier in the game. I grouped seasons into five-year eras (e.g., 2000-2004), then analyzed the time-to-replacement for all starting pitchers in the dataset.\n\n\n\nI found that pitchers in the 2015-2019 era were far more likely to be replaced earlier in the game, versus their counterparts in previous seasons. Below is an illustration of the ‘survival probability’, or the probability of remaining in the game for a starting pitcher from four different eras. It is clear that in the most-recent era (the purple line), starting pitchers had lower probabilities of remaining in the game at each number of batters faced (TBF). The p-value printed in the lower left is a result of comparing the four curves and assessing whether one or more curves differs from the rest. It is apparent that the difference in ‘survival’ in the 2015-2019 era, versus previous eras, was statistically significant.\n\n\n\nKaplan Meier curves illustrating probabilities of remaining in-game for starting pitchers in postseason games 2000-2019. The result of the log-rank test suggests a difference across season-eras.\n\n\nBelow are the model estimates for the effect of each covariate. For example, the Cox adjusted Hazard Ratio of Season era 2015-2019 (versus 2000-2004) was 2.69 (95% CI: 2.29 – 3.16). This means that starting pitchers in the most-recent era were 169% more likely to be replaced against any given batter, versus the 2000-2004 era. The relatively narrow confidence interval tells us that the estimated hazard ratio is reliable, i.e., if we were to replay history we would likely obtain a similar estimate.\n\n\n\nEstimated hazard ratios for the ‘replacement factors’ of starting pitchers. The replacement factors are thought to influence the timing of pitcher replacements in the postseason. A hazard ratio greater than one indicates a higher likelihood of pitcher replacement.\n\n\nTo illustrate the magnitude of the season-era effect, consider a hypothetical pitcher playing in both the 2000-2004 and the 2000-2019 eras. Assuming typical starter performance (3.8 pitches per batter faced, strikeout rate of 20%), this pitcher has a 70.3% chance of pitching a third time through the order in 2000-2004 but only a 38.8% chance of doing so in 2015-2019. With a runner in scoring position, these probabilities decline to 60.5% and 25.8%, respectively (with one home run conceded, the probabilities are similar). The probabilities below reflect the two scenarios, with the baseline scenario being no runners in scoring position (RISP) and no home runs conceded.\n\n\n\nPredicted probabilities of pitching a third time through the order for two hypothetical pitchers. The only difference between the pitchers is the era in which they play. The RISP scenario means at least one runner is in scoring position.\n\n\n\n\n\nThis work quantifies trends in starting pitcher replacements that have been formerly described, but not modeled in a time-to-event fashion. We can conclude that the strategy around starting pitcher replacement has been changing over the past 20 years of postseason play, since the effect of season-era is so significant. On the contrary, the use of the DH did not substantially alter the chance of pulling the starter when comparing managerial decisions across leagues.\nAside from the apparent trend of earlier staring pitcher replacements, there are several factors that indicate a meaningful impact on managerial strategy. For example, pitchers throwing more than 3.8 pitches per batter faced were more likely to be pulled (each additional pitch thrown per batter was associated with 127% higher chance of replacement). Similarly, a pitcher throwing to an opposite-handed batter was more likely to be pulled. Lastly, pitchers generating more than a 20% in-game strikeout rate were less likely to be pulled (each additional 10 percentage-point increase in K rate was associated with 28% lower chance of replacement).\nThe model has drawbacks, in that it does not capture head-to-head matchup history or differing styles of management. Those factors can be included in future work. First, for head-to-head matchup history, it would likely encompass an estimate of the on-base percentage of the batter against the specific pitcher in-question, observed during in the regular season of that same year. Ideally, this metric would be robust to small sample sizes, which are common when two opponents do not face each other very often in a season. The best framework for this would be an empirical Bayes estimate of head-to-head OBP.12\nSecondly, for managerial styles, it would make sense to specify a random effect to account for correlated outcomes from decisions made by the same manager. In other words, some managers would be likely to pull the starter early; others may have a tendency to leave him in. This involves a more advanced modeling approach but it is still possible within the broader Cox proportional hazards framework. An analysis that accounts for managerial styles would offer empirical evidence of any trends among certain managers that have been observed by the sports commentary community.\n\n\n\nPostseason play-by-play data was downloaded from RetroSheet’s postseason event files. In total, 611 games were included. Postseason rosters indicated the batting and throwing handedness of individual players. The use of a designated hitter was determined by the starting roster for each game.\nThe play-by-play data contain one row per ‘play’, where a play constitutes either a base-running event or a ball put into play by the batter (or both). To make data wrangling more manageable, all offesnive plays after the replacement of the opponent starting pitcher were discarded; this was done separately for each team. For the start of each batter’s plate appearance, several key factors were determined:\n\nBatter handedness: same as pitchers (yes/no)\nRunners in scoring position (yes/no)\nNumber of pitches thrown by stater (e.g., 0, 1, 2, …)\nHomeruns conceded by starter in the game\nPitcher strikeout rate in the game (e.g., 20%)\n\nImportantly, the above factors were determined without knowledge of the outcome of the batter’s plate appearance. Thus, the information used in modeling simulates all information available to a manager in deciding whether to replace a pitcher at the start of an at-bat.\nThe factors used to predict the replacement of a starting pitcher were called ‘replacement factors.’ In this analysis, only a limited set of replacement factors were considered. Note the average pitching volume was 3.8 pitches per batter faced; the average strikeout rate was 20%. To aid interpretability, the replacement factors for pitching volume and strikeout rate were centered at zero, therefore a positive value for either factor represents above-average.Above-average pitching volume is generally considered worse performance, as the pitcher must throw more times to the opponent and is expected to fatigues earlier. In contrast, above-average strikeout rate indicates elevated performance.\nOf the 1,218 starting pitching performances in the dataset, only 33 (2.7%) included complete game performances; complete games are very rare but not unheard of in the postseason.13 Fortunately the Cox proportional hazards framework does not require discarding complete games, even though the starting pitcher was never replaced. Instead, this is considered a censored observation, where we know that the pitcher remained in the game for its entirety. The underlying assumption is that the pitcher would have been replaced, for example if the game extended into extra innings. This is not an unreasonable assumption as even the best-throwing pitchers will eventually fatigue and need relief, if for no other reason than injury management."
  },
  {
    "objectID": "about.html#ray-pomponio",
    "href": "about.html#ray-pomponio",
    "title": "About",
    "section": "",
    "text": "Graduated May 2023, Master of Science in Biostatistics: University of Colorado (Denver, CO).\nBachelor of Science in Economics: University of Pennsylvania - Wharton (W’ 2018, Magna Cum Laude).\nExpertise in longitudinal data analysis and advanced statistical computing using R, SAS, and python.\nPast experience includes work in sports analytics, marketing, neuroimaging, and critical care medicine.\nOriginally from Philadelphia, PA.\nCurrently lives in Shadyside (Pittsburgh, PA).\nTo contact me, please reach me at: raymond.pomponio (at) outlook.com.\nFor freelance data science work, you can hire me on Upwork."
  },
  {
    "objectID": "draft.html",
    "href": "draft.html",
    "title": "Unpublished Draft",
    "section": "",
    "text": "The cumulative time played throughout a tennis tournament has been described as ‘competition load’.1 The effect of competition load on winning has been studied using a relatively simple model with data from Men’s quarterfinals, semifinals, and finals matches.2 With the identification a deleterious effect of higher load on the odds of winning, there remain many unanswered questions about the nuances of this result. Does competition load equate with fatigue, for example? Also, does competition load affect all players equally, or at least players of a similar age? One can imagine many potentially interesting findings from investigating answers to these questions alone.\n\n\n\nIn the 2022 US Open, Nick Kyrgios (Aus.) entered the quarterfinals as a favorite over Karen Khachanov (Russia). Kyrgios was ranked 25th at the time, while Khachanov was ranked 31st. Importantly, the two opponents had experienced differnt amounts of competition load up to that point in the tournament, with the higher-ranked Kyrgios having played 9.7 hours of matches through the first four rounds, compared with Khachanov’s 11 hours. Note, the typical load at this stage of a Men’s Grand Slam is about 9 hours and 23 minutes of on-court time.\nDid bettors miss an opportunity to capitalize on Khachanov’s odds of 4.35 going into the match, which he ultimately won, upsetting Kyrgios? A model trained to predict win probability suggests this match was closer-to-even than the odds suggested. In this model, I accounted for opponent ranks and opponent competition load, as well as court surface and tournament stakes. Based on the circumstances of the Kyrgios-Khachanov quarterfinal, the model predicts that Kyrgios had a 54.7% chance of winning, meaning the underdog Khachanov’s odds should have been closer to 2.21. A casual tennis fan, eyeing the rankings and time played in recent matches, might not have suspected that Khachanov could overcome his higher-than-average competition load of 11 hours to win the quarterfinal. Ultimately, Khachanov was defeated in the subsequent semifinal against 7th-ranked Casper Ruud (Norway).\n\n\n\nThis example from the 2022 US Open is illustrative of a possibly larger trend within professional tennis. That is, competition load can often favor the underdog in unexpected ways. It’s somewhat counterintuitive that a higher competition load contributes to higher odds for the lower-ranked player, while the same is not true for higher-ranked players. Specifically, each additional hour played by the lower-ranked player is associated with 3.6% higher odds of their winning the upcoming match, controlling for opponent load and strength (95 percent conf. interval: 2.2 — 5.0%).\nThe model remains to be validated in a true sportsbetting experiment, but current results are suggestive of several key insights. One, competition load may not be the same as fatigue, since lower-ranked players tend to perform better when entering matches with higher load. Secondly, the odds of an upcoming match may not optimally account for competition load, as we saw in the above example. Importantly, this analysis did not account for common player age effects nor player-specific responses to load. Those components will be part of future work.\nLinks: GitHub | Google Scholar"
  },
  {
    "objectID": "draft.html#modeling-the-timing-of-starting-pitcher-replacements-in-postseason-games-2023",
    "href": "draft.html#modeling-the-timing-of-starting-pitcher-replacements-in-postseason-games-2023",
    "title": "Work In Progress",
    "section": "",
    "text": "The following work has been submitted for consideration at the 2024 SABR Analytics Conference in Phoenix, Arizona: conference page.\n\n\nRecently a number of research papers have emerged examining the effects of starting pitcher replacements in Major League Baseball (MLB).1 2 3 This past year, the time through the order penalty (TTOP) was analyzed in a Bayesian framework and results were published in the Journal of Quantiative Analysis in Sports.4 There remain many questions about starting pitcher strategies, particularly in the postseason when managers’ decisions are hyper-scrutinized.5 Despite a trend towards earlier starting pitcher replacements in both regular season6 7 and postseason8 9 play, I am not aware of any analysis that has considered the replacement of the starting pitcher as a time-to-event phenomenon. The field of biostatistics has benefited greatly from the ability to model time-to-event outcomes, such as all-cause mortality, within a regression framework outlined by the late David Cox.10 Cox’s proportional hazards model is convenient for several reasons. It permits the formal modeling of a relationship between time and the event of interest; it incorporates covariates that increase or reduce the risk of the event; and, with modern software one can easily estimate the probability of an event occurring within a pre-specified time window. All of these characteristics are useful in the context of analyzing starting pitcher replacements. I will make the case that Cox’s model is more suitable for modeling pitcher replacements than alternative approaches using classification or supervised machine learning techniques.\n\n\n\nIn game seven of the 2003 ALCS, the Red Sox held a two-run lead over the Yankees in the eighth inning when manager Grady Little visited the mound.11 Starting pitcher Pedro Martínez was still in the game, with Hideki Matsui at the plate. Little decided to leave Martínez in for both Matsui and Jorge Posada, resulting in the loss of the lead and ultimately the game in 11 innings. At the time of Little’s mound visit, how likely was Martínez’s replacement? The model that is developed below suggests that Martínez would be pulled in 60.4% of games with similar circumstances.\n\n\n\nOther sabermetrics work has identified important in-game factors that predict a starter’s replacement, such as the strike count, pitch count, number of outs, total batters faced, and current home runs conceded. However, pitching replacements have not been modeled as time-to-event outcomes. Instead, they have been modeled as binary outcomes. The advantage of timing models is that the cumulative effect of time is inherent; certain factors can expedite, or delay, the timing of the event. This process is also more realistic than a binary outcomes process, which considers each event independently.\nThe first question in this analysis is, what is the unit of time? For studying outcomes in medicine, time is often measured in months or years. However, in a baseball game, no more than a few hours go by, and typically the starting pitcher is replaced mid-way through the game. I chose to use the number of batters faced as the unit of time in this analysis. While this is not a measure of clock time, it is a measure of the number of opponents that the pitcher has faced. The higher the number of opponents, the more fatigued the pitcher will become. Alternatively one could consider the number of pitches thrown as the unit of time. I don’t see any issue with this, though pitcher replacements often happen between batter plate appearanced, and pitch-by-pitch data is much more granular, potentially complicating the analysis\nFor data, I used all postseason games from 2000-2019 (source: RetroSheet.org). There were several reasons for choosing these seasons and not including more recent seasons. First, the obvious complications of the COVID-19 pandemic make working with the 2020 postseason challenging. Secondly, a number of recent rule changes have been implemented which might influence the strategy around starting pitcher replacement, for example the pitch clock. Therefore I consider the 20-year window between 2000 and 2019 to be a relatively stable period of postseason play. Future work will have to revisit the same questions explored here and see if the underlying conclusions hold up in the modern era.\nI used Cox’s proportional hazards modeling approach and controlled for in-game strikeout rate, pitching volume (pitches thrown per batter faced), runners in scoring position, and home runs conceded, as well as differences across season eras and leagues. All of the covariates included in the model were computed sequentially for each batter faced. In other words, I did not use data from later in the game to inform a covariate value earlier in the game. I grouped seasons into five-year eras (e.g., 2000-2004), then analyzed the time-to-replacement for all starting pitchers in the dataset.\n\n\n\nI found that pitchers in the 2015-2019 era were far more likely to be replaced earlier in the game, versus their counterparts in previous seasons. Below is an illustration of the ‘survival probability’, or the probability of remaining in the game for a starting pitcher from four different eras. It is clear that in the most-recent era (the purple line), starting pitchers had lower probabilities of remaining in the game at each number of batters faced (TBF). The p-value printed in the lower left is a result of comparing the four curves and assessing whether one or more curves differs from the rest. It is apparent that the difference in ‘survival’ in the 2015-2019 era, versus previous eras, was statistically significant.\n\n\n\nKaplan Meier curves illustrating probabilities of remaining in-game for starting pitchers in postseason games 2000-2019. The result of the log-rank test suggests a difference across season-eras.\n\n\nBelow are the model estimates for the effect of each covariate. For example, the Cox adjusted Hazard Ratio of Season era 2015-2019 (versus 2000-2004) was 2.69 (95% CI: 2.29 – 3.16). This means that starting pitchers in the most-recent era were 169% more likely to be replaced against any given batter, versus the 2000-2004 era. The relatively narrow confidence interval tells us that the estimated hazard ratio is reliable, i.e., if we were to replay history we would likely obtain a similar estimate.\n\n\n\nEstimated hazard ratios for the ‘replacement factors’ of starting pitchers. The replacement factors are thought to influence the timing of pitcher replacements in the postseason. A hazard ratio greater than one indicates a higher likelihood of pitcher replacement.\n\n\nTo illustrate the magnitude of the season-era effect, consider a hypothetical pitcher playing in both the 2000-2004 and the 2000-2019 eras. Assuming typical starter performance (3.8 pitches per batter faced, strikeout rate of 20%), this pitcher has a 70.3% chance of pitching a third time through the order in 2000-2004 but only a 38.8% chance of doing so in 2015-2019. With a runner in scoring position, these probabilities decline to 60.5% and 25.8%, respectively (with one home run conceded, the probabilities are similar). The probabilities below reflect the two scenarios, with the baseline scenario being no runners in scoring position (RISP) and no home runs conceded.\n\n\n\nPredicted probabilities of pitching a third time through the order for two hypothetical pitchers. The only difference between the pitchers is the era in which they play. The RISP scenario means at least one runner is in scoring position.\n\n\n\n\n\nThis work quantifies trends in starting pitcher replacements that have been formerly described, but not modeled in a time-to-event fashion. We can conclude that the strategy around starting pitcher replacement has been changing over the past 20 years of postseason play, since the effect of season-era is so significant. On the contrary, the use of the DH did not substantially alter the chance of pulling the starter when comparing managerial decisions across leagues.\nAside from the apparent trend of earlier staring pitcher replacements, there are several factors that indicate a meaningful impact on managerial strategy. For example, pitchers throwing more than 3.8 pitches per batter faced were more likely to be pulled (each additional pitch thrown per batter was associated with 127% higher chance of replacement). Similarly, a pitcher throwing to an opposite-handed batter was more likely to be pulled. Lastly, pitchers generating more than a 20% in-game strikeout rate were less likely to be pulled (each additional 10 percentage-point increase in K rate was associated with 28% lower chance of replacement).\nThe model has drawbacks, in that it does not capture head-to-head matchup history or differing styles of management. Those factors can be included in future work. First, for head-to-head matchup history, it would likely encompass an estimate of the on-base percentage of the batter against the specific pitcher in-question, observed during in the regular season of that same year. Ideally, this metric would be robust to small sample sizes, which are common when two opponents do not face each other very often in a season. The best framework for this would be an empirical Bayes estimate of head-to-head OBP.12\nSecondly, for managerial styles, it would make sense to specify a random effect to account for correlated outcomes from decisions made by the same manager. In other words, some managers would be likely to pull the starter early; others may have a tendency to leave him in. This involves a more advanced modeling approach but it is still possible within the broader Cox proportional hazards framework. An analysis that accounts for managerial styles would offer empirical evidence of any trends among certain managers that have been observed by the sports commentary community.\n\n\n\nPostseason play-by-play data was downloaded from RetroSheet’s postseason event files. In total, 611 games were included. Postseason rosters indicated the batting and throwing handedness of individual players. The use of a designated hitter was determined by the starting roster for each game.\nThe play-by-play data contain one row per ‘play’, where a play constitutes either a base-running event or a ball put into play by the batter (or both). To make data wrangling more manageable, all offesnive plays after the replacement of the opponent starting pitcher were discarded; this was done separately for each team. For the start of each batter’s plate appearance, several key factors were determined:\n\nBatter handedness: same as pitchers (yes/no)\nRunners in scoring position (yes/no)\nNumber of pitches thrown by stater (e.g., 0, 1, 2, …)\nHomeruns conceded by starter in the game\nPitcher strikeout rate in the game (e.g., 20%)\n\nImportantly, the above factors were determined without knowledge of the outcome of the batter’s plate appearance. Thus, the information used in modeling simulates all information available to a manager in deciding whether to replace a pitcher at the start of an at-bat.\nThe factors used to predict the replacement of a starting pitcher were called ‘replacement factors.’ In this analysis, only a limited set of replacement factors were considered. Note the average pitching volume was 3.8 pitches per batter faced; the average strikeout rate was 20%. To aid interpretability, the replacement factors for pitching volume and strikeout rate were centered at zero, therefore a positive value for either factor represents above-average.Above-average pitching volume is generally considered worse performance, as the pitcher must throw more times to the opponent and is expected to fatigues earlier. In contrast, above-average strikeout rate indicates elevated performance.\nOf the 1,218 starting pitching performances in the dataset, only 33 (2.7%) included complete game performances; complete games are very rare but not unheard of in the postseason.13 Fortunately the Cox proportional hazards framework does not require discarding complete games, even though the starting pitcher was never replaced. Instead, this is considered a censored observation, where we know that the pitcher remained in the game for its entirety. The underlying assumption is that the pitcher would have been replaced, for example if the game extended into extra innings. This is not an unreasonable assumption as even the best-throwing pitchers will eventually fatigue and need relief, if for no other reason than injury management."
  },
  {
    "objectID": "draft.html#footnotes",
    "href": "draft.html#footnotes",
    "title": "Unpublished Draft",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nS. Kovalchik, “Did Competition Load Help Make Medvedev a Grand Slam Champion?,” Stats on the T, 31 January 2022 [Online]. http://on-the-t.com/2021/09/12/Did-Competition-Load-Help-Medvedev/.↩︎\nS. Kovalchik, “Does greater competition load going into the second week of a Grand Slam hurt a player’s win chances?,” Stats on the T, 15 February 2020 [Online]. http://on-the-t.com/2020/02/15/competition-load/.↩︎"
  },
  {
    "objectID": "draft.html#impact-of-competition-load-on-winning-in-mens-tennis",
    "href": "draft.html#impact-of-competition-load-on-winning-in-mens-tennis",
    "title": "Unpublished Draft",
    "section": "",
    "text": "The cumulative time played throughout a tennis tournament has been described as ‘competition load’.1 The effect of competition load on winning has been studied using a relatively simple model with data from Men’s quarterfinals, semifinals, and finals matches.2 With the identification a deleterious effect of higher load on the odds of winning, there remain many unanswered questions about the nuances of this result. Does competition load equate with fatigue, for example? Also, does competition load affect all players equally, or at least players of a similar age? One can imagine many potentially interesting findings from investigating answers to these questions alone.\n\n\n\nIn the 2022 US Open, Nick Kyrgios (Aus.) entered the quarterfinals as a favorite over Karen Khachanov (Russia). Kyrgios was ranked 25th at the time, while Khachanov was ranked 31st. Importantly, the two opponents had experienced differnt amounts of competition load up to that point in the tournament, with the higher-ranked Kyrgios having played 9.7 hours of matches through the first four rounds, compared with Khachanov’s 11 hours. Note, the typical load at this stage of a Men’s Grand Slam is about 9 hours and 23 minutes of on-court time.\nDid bettors miss an opportunity to capitalize on Khachanov’s odds of 4.35 going into the match, which he ultimately won, upsetting Kyrgios? A model trained to predict win probability suggests this match was closer-to-even than the odds suggested. In this model, I accounted for opponent ranks and opponent competition load, as well as court surface and tournament stakes. Based on the circumstances of the Kyrgios-Khachanov quarterfinal, the model predicts that Kyrgios had a 54.7% chance of winning, meaning the underdog Khachanov’s odds should have been closer to 2.21. A casual tennis fan, eyeing the rankings and time played in recent matches, might not have suspected that Khachanov could overcome his higher-than-average competition load of 11 hours to win the quarterfinal. Ultimately, Khachanov was defeated in the subsequent semifinal against 7th-ranked Casper Ruud (Norway).\n\n\n\nThis example from the 2022 US Open is illustrative of a possibly larger trend within professional tennis. That is, competition load can often favor the underdog in unexpected ways. It’s somewhat counterintuitive that a higher competition load contributes to higher odds for the lower-ranked player, while the same is not true for higher-ranked players. Specifically, each additional hour played by the lower-ranked player is associated with 3.6% higher odds of their winning the upcoming match, controlling for opponent load and strength (95 percent conf. interval: 2.2 — 5.0%).\nThe model remains to be validated in a true sportsbetting experiment, but current results are suggestive of several key insights. One, competition load may not be the same as fatigue, since lower-ranked players tend to perform better when entering matches with higher load. Secondly, the odds of an upcoming match may not optimally account for competition load, as we saw in the above example. Importantly, this analysis did not account for common player age effects nor player-specific responses to load. Those components will be part of future work."
  }
]