[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Ray Pomponio is a data scientist. He received his Master’s degree in Biostatistics from the University of Colorado in 2023.\nRay has contributed to team science collaborations and provided biostatistical expertise/support for grant writing, study design, and longitudinal analysis. He has co-authored 20+ manuscripts in peer-reviewed journals.\nPlease reach out using any of the links below, or via email: raymond.pomponio (at) outlook.com."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A blog and stuff",
    "section": "",
    "text": "Rethinking Some Statistical Principles\n\n\n\n\n\n\nreading\n\n\nreflections\n\n\n\n\n\n\n\n\n\nApr 16, 2025\n\n\nRay Pomponio\n\n\n\n\n\n\n\n\n\n\n\n\nA Tool to Visualize Outdoor Climbing Performance\n\n\n\n\n\n\nhobbies\n\n\nadventure\n\n\n\n\n\n\n\n\n\nNov 27, 2024\n\n\nRay Pomponio\n\n\n\n\n\n\n\n\n\n\n\n\nA Summary of My Research in Stats and Medicine\n\n\n\n\n\n\npersonal\n\n\nreflections\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\nRay Pomponio\n\n\n\n\n\n\n\n\n\n\n\n\nStarting Pitcher Replacements in the Postseason: Time-to-event methods\n\n\n\n\n\n\nanalysis\n\n\nsports\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nRay Pomponio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/research-summary/index.html",
    "href": "posts/research-summary/index.html",
    "title": "A Summary of My Research in Stats and Medicine",
    "section": "",
    "text": "In this post, I review progress in my research so far, and distill my work into three themes spanning topics in biomedical sciences and applied statistics."
  },
  {
    "objectID": "posts/research-summary/index.html#theme-1-computational-research",
    "href": "posts/research-summary/index.html#theme-1-computational-research",
    "title": "A Summary of My Research in Stats and Medicine",
    "section": "Theme 1: Computational Research",
    "text": "Theme 1: Computational Research\n\n\n\nFigure from Pomponio et al. (2020): Age trends from selected brain volumes using a statistically harmonized dataset with 18 studies spanning the age range 3–96.\n\n\n\nComputational Neuroimaging and Related Challenges\nIn 2018, I began working at CBICA, a computational neuroimaging lab at the University of Pennsylvania, led by Dr. Christos Davatzikos. At CBICA, I developed an early interest in statistical image analysis, particularly in normalizing imaging-derived features over multiple scanners and diverse geographic sites. In the imaging literature, this is referred to as harmonization, and there are a growing number of techniques available to implement harmonization. My most significant contribution was in developing a tool that adapts the popular ComBat1 algorithm to lifespan datasets encompassing broad age ranges, with potentially complex age trends in brain volumetry. Today, that tool is available on GitHub as a python package, and it is used by neuroimaging researchers to handle common harmonization tasks like removing site effects in regional brain volumes data.\nIn addition to my work on harmonization, I assisted with image analysis in several other projects that resulted in peer-reviewed publications from the following first authors:\n\nDr. Ilya M. Nasrallah, Associate Professor of Radiology at the Hospital of the University of Pennsylvania\nDr. Mohamad Habes, Assistant Professor, Departments of Radiology and Epidemiology at the University of Texas Health Science Center San Antonio\nDr. Ganesh Chand, Assistant Professor at Washington University in St. Louis\nVishnu M. Bashyam, PhD student at the University of Pennsylvania\n\n\n\nPublished Papers\n\n  \n    \n      \n        Harmonization of large MRI datasets for the analysis of brain imaging patterns throughout the lifespan (NeuroImage 2020)\n      \n    \n    \n      \n        Co-Authors: Raymond Pomponio, Guray Erus, Mohamad Habes, Jimit Doshi, Dhivya Srinivasan, Elizabeth Mamourian, Vishnu Bashyam, Ilya M Nasrallah, Theodore D Satterthwaite, Yong Fan, Lenore J Launer, Colin L Masters, Paul Maruff, Chuanjun Zhuo, Henry Völzke, Sterling C Johnson, Jurgen Fripp, Nikolaos Koutsouleris, Daniel H Wolf, Raquel Gur, Ruben Gur, John Morris, Marilyn S Albert, Hans J Grabe, Susan M Resnick, R Nick Bryan, David A Wolk, Russell T Shinohara, Haochang Shou, Christos Davatzikos. \n        \n        DOI: Click to read article.\n      \n    \n  \n  \n    \n      \n        Two distinct neuroanatomical subtypes of schizophrenia revealed using machine learning (Brain, 2020)\n      \n    \n    \n      \n        Co-Authors: Ganesh B Chand, Dominic B Dwyer, Guray Erus, Aristeidis Sotiras, Erdem Varol, Dhivya Srinivasan, Jimit Doshi, Raymond Pomponio, Alessandro Pigoni, Paola Dazzan, Rene S Kahn, Hugo G Schnack, Marcus V Zanetti, Eva Meisenzahl, Geraldo F Busatto, Benedicto Crespo-Facorro, Christos Pantelis, Stephen J Wood, Chuanjun Zhuo, Russell T Shinohara, Haochang Shou, Yong Fan, Ruben C Gur, Raquel E Gur, Theodore D Satterthwaite, Nikolaos Koutsouleris, Daniel H Wolf, Christos Davatzikos. \n        \n        DOI: Click to read article.\n      \n    \n  \n  \n    \n      \n        MRI signatures of brain age and disease over the lifespan based on a deep brain network and 14,468 individuals worldwide (Brain, 2020)\n      \n    \n    \n      \n        Co-Authors: Vishnu M Bashyam, Guray Erus, Jimit Doshi, Mohamad Habes, Ilya M Nasrallah, Monica Truelove-Hill, Dhivya Srinivasan, Liz Mamourian, Raymond Pomponio, Yong Fan, Lenore J Launer, Colin L Masters, Paul Maruff, Chuanjun Zhuo, Henry Völzke, Sterling C Johnson, Jurgen Fripp, Nikolaos Koutsouleris, Theodore D Satterthwaite, Daniel Wolf, Raquel E Gur, Ruben C Gur, John Morris, Marilyn S Albert, Hans J Grabe, Susan Resnick, R Nick Bryan, David A Wolk, Haochang Shou, Christos Davatzikos. \n        \n        DOI: Click to read article.\n      \n    \n  \n  \n    \n      \n        The Brain Chart of Aging: Machine-learning analytics reveals links between brain aging, white matter disease, amyloid burden, and cognition in the iSTAGING consortium of 10,216 harmonized MR scans (Alzheimer's & Dementia, 2020)\n      \n    \n    \n      \n        Co-Authors: Mohamad Habes, Raymond Pomponio, Haochang Shou, Jimit Doshi, Elizabeth Mamourian, Guray Erus, Ilya Nasrallah, Lenore J Launer, Tanweer Rashid, Murat Bilgel, Yong Fan, Jon B Toledo, Kristine Yaffe, Aristeidis Sotiras, Dhivya Srinivasan, Mark Espeland, Colin Masters, Paul Maruff, Jurgen Fripp, Henry Völzk, Sterling C Johnson, John C Morris, Marilyn S Albert, Michael I Miller, R Nick Bryan, Hans J Grabe, Susan M Resnick, David A Wolk, Christos Davatzikos, iSTAGING consortium, the Preclinical AD consortium, the ADNI, and the CARDIA studies. \n        \n        DOI: Click to read article.\n      \n    \n  \n  \n    \n      \n        Association of Intensive vs Standard Blood Pressure Control With Magnetic Resonance Imaging Biomarkers of Alzheimer Disease: Secondary Analysis of the SPRINT MIND Randomized Trial (JAMA Neurology, 2021)\n      \n    \n    \n      \n        Co-Authors: Ilya M Nasrallah, Sarah A Gaussoin, Raymond Pomponio, Sudipto Dolui, Guray Erus, Clinton B Wright, Lenore J Launer, John A Detre, David A Wolk, Christos Davatzikos, Jeff D Williamson, Nicholas M Pajewski, R Nick Bryan, Paul Whelton, Karen C Johnson, Joni Snyder, Diane Bild, Denise Bonds, Nakela Cook, Jeffrey Cutler, Lawrence Fine, Peter Kaufmann, Paul Kimmel, Lenore Launer, Claudia Moy, William Riley, Laurie Ryan, Eser Tolunay, Song Yang, David Reboussin, Walter T Ambrosius, William Applegate, Greg Evans, Capri Foy, Barry I Freedman, Dalane Kitzman, Mary Lyles, Steve Rapp, Scott Rushing, Neel Shah, Kaycee M Sink, Mara Vitolins, Lynne Wagenknecht, Valerie Wilson, Letitia Perdue, Nancy Woolard, Tim Craven, Katelyn Garcia, Sarah Gaussoin, Laura Lovato, Jill Newman, James Lovato, Lingyi Lu, Chris McLouth, Greg Russell, Bobby Amoroso, Patty Davis, Jason Griffin, Darrin Harris, Mark King, Kathy Lane, Wes Roberson, Debbie Steinberg, Donna Ashford, Phyllis Babcock, Dana Chamberlain, Vickie Christensen, Loretta Cloud, Christy Collins, Delilah Cook, Katherine Currie, Debbie Felton, Stacy Harpe, Marjorie Howard, Michelle Lewis, Pamela Nance, Nicole Puccinelli-Ortega, Laurie Russell, Jennifer Walker, Brenda Craven, Candace Goode, Margie Troxler, Janet Davis, Sarah Hutchens, Anthony A Killeen, Anna M Lukkari, Robert Ringer, Brandi Dillard, Norbert Archibeque, Stuart Warren, Mike Sather, James Pontzer, Zach Taylor, Elsayed Z Soliman, Zhu-Ming Zhang, Yabing Li, Chuck Campbell, Susan Hensley, Julie Hu, Lisa Keasler, Mary Barr, Tonya Taylor, Ilya Nasrallah, Lisa Desiderio, Mark Elliott, Ari Borthakur, Harsha Battapady, Alex Smith, Ze Wang, Jimit Doshi, Jackson T Wright Jr, Mahboob Rahman, Alan J Lerner, Carolyn H Still, Alan Wiggers, Sara Zamanian, Alberta Bee, Renee Dancie, George Thomas, Martin Schreiber Jr, Sankar Dass Navaneethan, John Hickner, Michael Lioudis, Michelle Lard, Susan Marczewski, Jennifer Maraschky, Martha Colman, Andrea Aaby, Stacey Payne, Melanie Ramos, Carol Horner, Paul E Drawz, Pratibha P Raghavendra, Scott Ober, Ronda Mourad, Muralidhar Pallaki, Peter Russo, Paul Fantauzzo, Lisa Tucker, Bill Schwing, John R Sedor, Edward J Horwitz, Jeffrey R Schellling, John F O’Toole, Lisa Humbert, Wendy Tutolo, Suzanne White, Alishea Gay, Walter Clark Jr, Robin Hughes. \n        \n        DOI: Click to read article."
  },
  {
    "objectID": "posts/research-summary/index.html#theme-2-clinical-research",
    "href": "posts/research-summary/index.html#theme-2-clinical-research",
    "title": "A Summary of My Research in Stats and Medicine",
    "section": "Theme 2: Clinical Research",
    "text": "Theme 2: Clinical Research\n\nPulmonary and Critical Care Collaborations\nIn 2021, under Dr. Ryan Peterson, I joined Colorado School of Public Health’s PTraC Team, which specializes in statistical research services for the Division of Pulmonary and Critical Care Medicine at Colorado. I was fortunate to work with excellent clinical collaborators at PTraC, including specialists in lung transplant, severe asthma, and pulmonary hypertension. My work was characterized by general applications of regression models to observational data, meaning data were collected by recording clinical events (as opposed to data collected from controlled experiments). Data types were highly heterogeneous and included electronic health records, insurance claims, and a national disease registry. Among the more common methods I used were survival analysis and longitudinal modeling, two popular frameworks for fitting regressions to biomedical datasets. While at PTraC, I was selected to present a lightning talk on my work at the 2022 Symposium on Statistics and Data Science in Pittsburgh.\n\n\nPublished and Accepted Papers\n\n  \n    \n      \n        Impact of the COVID‐19 pandemic on chronic disease management and patient reported outcomes in patients with pulmonary hypertension: The Pulmonary Hypertension Association Registry (Pulmonary Circulation 2023)\n      \n    \n    \n      \n        Co-Authors: Megan Mayer, David B Badesch, Kelly H Nielsen, Steven Kawut, Todd Bull, John J Ryan, Jeffrey Sager, Sula Mazimba, Anna Hemnes, James Klinger, James Runo, John W McConnell, Teresa De Marco, Murali M Chakinala, Delphine Yung, Jean Elwing, Adolfo Kaplan, Rahul Argula, Raymond Pomponio, Ryan Peterson, Peter Hountras. \n        \n        DOI: Click to read article.\n      \n    \n  \n  \n    \n      \n        Prevalence of Alcohol Use Characterized by Phosphatidylethanol in Patients with Respiratory Failure Before and During the COVID-19 Pandemic (CHEST Critical Care, 2024)\n      \n    \n    \n      \n        Note: This article has been accepted but is not yet published.\n      \n    \n  \n  \n    \n      \n        Extraneous Load, Patient Census, and Patient Acuity Correlate with Cognitive Load during Intensive Care Unit Rounds (CHEST, 2024)\n      \n    \n    \n      \n        Note: This article has been accepted but is not yet published."
  },
  {
    "objectID": "posts/research-summary/index.html#theme-3-missing-data-methods",
    "href": "posts/research-summary/index.html#theme-3-missing-data-methods",
    "title": "A Summary of My Research in Stats and Medicine",
    "section": "Theme 3: Missing Data Methods",
    "text": "Theme 3: Missing Data Methods\n\n\n\nFigure from Pomponio et al. (2023): Diagram illustrating matched data, partially matched data, and unmatched data scenarios.\n\n\nThe last theme of my research deals with missing data. Missing data are common yet complicated by enormous heterogeneity; missing data can arise at various stages within a study, and they present so many challenges that practitioners often ignore missing data altogether. In my time at Colorado, I encountered missing data in observational studies due to participant dropout or early mortality. When working with this kind of missing data, I often used multiple imputation by chained equations2 to replace missing observations with values that would be realistic given the remaining data. This technique works well when certain assumptions are met and is conveniently implemented inR by the mice package.\nHowever, another kind of missing data can arise in studies investigating pre-post differences that fail to collect unique identifiers for each participant. For my master’s thesis at Colorado, I analyzed techniques to handle this particular case of paired inference, which we refer to as ‘partially matched’ data. My work on this subject is currently under review, but the pre-print is linked below, co-authored by the following members of my thesis committee:\n\nDr. Ryan A. Peterson, Assistant Professor of Biostatistics and Informatics at the Colorado School of Public Health\nDr. Bailey K. Fosdick, Associate Professor in the Department of Biostatistics and Informatics in the Colorado School of Public Health\nDr. Julia Wrobel, Assistant Professor of Biostatistics at Emory University\n\n\nPreprint Papers\n\n  \n    \n      \n        Mistaken identities lead to missed opportunities: Testing for mean differences in partially matched data (Under Review)\n      \n    \n    \n      \n        Co-Authors: Raymond Pomponio, Bailey K. Fosdick, Julia Wrobel, Ryan A. Peterson. \n        \n        DOI: Click to read preprint."
  },
  {
    "objectID": "posts/research-summary/index.html#conclusions-and-next-steps",
    "href": "posts/research-summary/index.html#conclusions-and-next-steps",
    "title": "A Summary of My Research in Stats and Medicine",
    "section": "Conclusions and Next Steps",
    "text": "Conclusions and Next Steps\nLooking back at the past five years, it’s clear that I’ve primarily worked on academic research projects focusing on biomedical diseases and health interventions. Broadly, I’ve developed an expertise as a biostatistician and data scientist in the medical research context.\nIf I am to speculate on the direction of the next five years, I would predict much of the same kind of work, but with expansions into new contexts such as business and sports analytics. These areas excite me by presenting an opportunity to integrate my undergraduate business experience with cutting-edge data science methods. I’ve begun to work a little in these areas as an independent consultant through Upwork (a freelancing platform), and I hope that I can continue to find work with clients who value my insight in diverse and wide-ranging projects.\nIn 2024, I look forward to beginning a new position as a data scientist at SygnaMap, a bio-tech startup based in San Antonio. At SygnaMap I will be helping to develop novel biomarkers using metabolomic profiles, which will draw on many of my past experiences including the three themes of computational complexity, clinical utility, and challenges arising from missing data."
  },
  {
    "objectID": "posts/research-summary/index.html#footnotes",
    "href": "posts/research-summary/index.html#footnotes",
    "title": "A Summary of My Research in Stats and Medicine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nW.E. Johnson, C. Li, A. Rabinovic. Adjusting batch effects in mircoarray expression data using empirical Bayes methods. Biostatistics, 8 (2007), pp. 118-127. https://doi.org/10.1093/biostatistics/kxj037.↩︎\nAzur, M.J., Stuart, E.A., Frangakis, C. and Leaf, P.J. (2011), Multiple imputation by chained equations: what is it and how does it work?. Int. J. Methods Psychiatr. Res., 20: 40-49. https://doi.org/10.1002/mpr.329.↩︎"
  },
  {
    "objectID": "posts/statistical-rethinking/index.html",
    "href": "posts/statistical-rethinking/index.html",
    "title": "Rethinking Some Statistical Principles",
    "section": "",
    "text": "I’ve begun reading Statistical Rethinking by Richard McElreath,1 and I want to log some of my thoughts that came to mind while working through his ideas on modern statistical science."
  },
  {
    "objectID": "posts/statistical-rethinking/index.html#likelihoods",
    "href": "posts/statistical-rethinking/index.html#likelihoods",
    "title": "Rethinking Some Statistical Principles",
    "section": "Likelihoods",
    "text": "Likelihoods\nIn this chapter, a hypothetical experiment was introduced in which an observer collects data about the proportion of water on a globe (to understand the true proportion of water on earth).\nLet \\(p\\) denote the proportion of water on the globe, \\(W\\) the number of observations that yielded “water”, and \\(L\\) the number of observations that yielded “land”. By the Binomial distribution:\n\\[\nPr(W=w,L=l\\quad|\\quad p)={w+l\\choose w}p^w(1-p)^l\n\\]\nNow presume the observer recorded \\(W=6\\) and \\(L=3\\) in nine trials. The “likelihood” of these data can be plotted as a function of probability:\n\n\n\n\n\n\n\n\n\nAnd this is by no means a probability distribution. It doesn’t sum to one; it integrates to 0.1. However the recognition of this function is a crucial step towards two possible modes of inference:\n\nIn a classical sense, this function represents an objective to maximize, then compare against a null hypothesis (often \\(H_0: p=0.5\\)).\nIn a Bayesian sense, this function represents one piece of the joint distribution of \\(W, L, p\\) (the other piece being the prior on \\(p\\)). And this is the mode of inference advocated by the book.\n\nWhen we start with a uniform prior \\(p\\sim\\text{Beta}(1, 1)\\), we end up with a posterior that looks uncannily like our likelihood function:\n\nPosteriorLikelihood (again)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThey’re actually the same function, just with different y-axis scales. I previously didn’t realize that…\n\n\n\n\n\n\nLikelihoods as Objectives\n\n\n\nAs part of the classical mode of inference, one might reasonably seek to maximize the above likelihood function \\(\\mathcal{L}\\), setting \\(p=\\hat{p}\\) at maximal likelihood, then comparing the ratio of likelihoods between \\(\\mathcal{L}(\\hat{p})\\) and \\(\\mathcal{L}(p_0)\\). It turns out, when this ratio is greater than some constant \\(c\\), one would rationally conclude that \\(p\\neq p_0\\) (in other words, one rejects the null hypothesis).\n\n\nSo contrasting the two modes of inference: in the classical sense, one compares two points along the likelihood curve to posit evidence against some hypothesis \\(H_0\\). In a Bayesian sense, one obtains a posterior distribution over plausible values of \\(p\\), which can be further analyzed for compatibility with any set of hypotheses. And the realization for me (at least in this trivial example) is that the likelihood and posterior functions are the exact same shape.\nReflecting on this a little more, I can understand how the relative simplicity of the classical approach might be appealing at the start of a scientific inquiry, say, when we don’t know much about the phenomenon but expect to learn a lot quickly. However, when an inquiry matures to the point of having data readily available, it seems silly to forgo the benefit of having the entire distribution of a parameter available to interrogate. It’s clear that the latter offers a richer description of the unknown phenomenon. Perhaps an analogy would be like entering a dark cavernous tunnel, where the classical approach affords you looks at two possible paths forward (one of which is a dead-end and the other quite a promising lead). The Bayesian approach, on the other hand, affords you something like a map of multiple paths forward."
  },
  {
    "objectID": "posts/statistical-rethinking/index.html#algorithms-for-posterior-approximation",
    "href": "posts/statistical-rethinking/index.html#algorithms-for-posterior-approximation",
    "title": "Rethinking Some Statistical Principles",
    "section": "Algorithms for Posterior Approximation",
    "text": "Algorithms for Posterior Approximation\nI was previously aware of posterior approximation using Markov chain Monte Carlo (MCMC), but I was surprised to learn about two viable alternatives to numerically sampling from a posterior distribution.\nI’ll continue with the hypothetical experiment from the previous section in which we are aiming to estimate \\(p\\), the proportion of water on the globe.\n\nGrid ApproximationQuadratic ApproximationMCMC\n\n\nBy selecting a finite grid across the support of our parameter \\(p\\), we can approximate the posterior:\n\n\nCode\ngrid.seq &lt;- seq(0, 1, length.out=20)\nprior &lt;- dunif(grid.seq)\nlikelihood &lt;- dbinom(6, size=9, prob=grid.seq)\nposterior &lt;- prior * likelihood / sum(prior * likelihood)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis technique involves leveraging a Gaussian approximation of the posterior near its mode. I believe the theory behind it is related to the efficiency property2 of MLEs, which states that the MLE converges in distribution to a normal distribution at larger sample sizes. It is relatively easy to code up using the quap function from the textbook’s rethinking package:3\n\n\nCode\nposterior &lt;- quap(\n  alist(\n    W ~ dbinom(W+L ,p) ,\n    p ~ dunif(0, 1)),\n  data=list(W=6,L=3) )\nquad.post &lt;- precis(posterior)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally MCMC, which is the most-widely adopted approach to posterior approximation, has many algorithmic variants. The following is an example of the Metropolis Algorithm:4\n\n\nCode\nset.seed(1818)\nn.samples &lt;- 2000\np &lt;- rep(NA, n.samples)\np[1] &lt;- 0.5\nobs.data &lt;- list(W=6, L=3)\nfor (i in 2:n.samples){\n  p.new &lt;- rnorm(1 , p[i - 1] , 0.1)\n  if (p.new &lt; 0) p.new &lt;- abs(p.new)\n  if (p.new &gt; 1) p.new &lt;- 2 - p.new\n  q0 &lt;- dbinom(obs.data$W, obs.data$W + obs.data$L, p[i-1])\n  q1 &lt;- dbinom(obs.data$W, obs.data$W + obs.data$L, p.new)\n  p[i] &lt;- ifelse(runif(1) &lt; q1/q0, p.new, p[i-1])\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe one intriguing feature of the Metropolis Algorithm is the acceptance rule, which computes a ratio of likelihoods, given \\(p\\) and \\(p'\\), and accepts the new value \\(p'\\) if this ratio is above some randomly-generated value.\nIn other words, the internal mechanics of the algorithm work a bit like the classical approach to inference (see above), in which the likelihoods of two points are compared. In the classical approach, \\(\\hat{p}\\) is never technically “accepted”, but \\(p_0\\) can be rejected if the ratio of likelihoods (\\(\\mathcal{L}(\\hat{p})\\) vs. \\(\\mathcal{L}(p_0)\\)) is above some constant \\(c\\). In contrast to MCMC, however, \\(c\\) is not randomly-generate but instead dictated by the choice of \\(\\alpha\\), which is commonly 0.05.\nAnother noteworthy feature, not highlighted here, is that the Metropolis Algorithm does not require computation of exact densities; relative density functions work just fine. That’s a major advantage when it comes time to scale the algorithm with increasing model complexity."
  },
  {
    "objectID": "posts/statistical-rethinking/index.html#normal-gaussian-distributions",
    "href": "posts/statistical-rethinking/index.html#normal-gaussian-distributions",
    "title": "Rethinking Some Statistical Principles",
    "section": "Normal (Gaussian) Distributions",
    "text": "Normal (Gaussian) Distributions\nThis chapter points out the commonality of the Normal Distribution, which serves as a wonderful approximation of natural phenomena such as variation in height, variation in growth rates, or games of chance. The chapter begins by introducing a hypothetical experiment, which I paraphrase below:\n\nSuppose 1,000 people line up on the 50-yard line of a American football field. Each person flips a coin 16 times; for each time they see “heads” they step one yard in the direction of the home team’s endzone. Likewise, for each time they see “tails” the step one yard towards the away team’s endzone.\n\nIt’s apparent from the simulation below that this somewhat-contrived process will yield a distribution of yardage positions that looks like the following:\n\nN &lt;- 16\nset.seed(1818)\nsimulation &lt;- replicate(\n  1000,{\n  heads &lt;- rbinom(1, size=N, prob=0.5)\n  tails &lt;- N - heads\n  steps &lt;- heads - tails\n  steps\n  }\n)\nbarplot(prop.table(table(simulation)), xlab=\"Position (0=50-yard line)\")\nmtext(paste(\"Observed SD:\", round(sd(simulation), 3)))\n\n\n\n\n\n\n\n\nBy the central limit theorem, it’s possible to predict the ultimate distribution of positions. Let \\(X_i\\) be an individual coin flip, and \\(\\bar{X}\\) be the mean number of heads for an individual person; there are 1,000 people and \\(n=16\\) tosses per person. The central limit theorem predicts the distribution of \\(Y=n(2\\bar{X}-1)\\) will converge towards:\n\\[\nY \\rightarrow N(0, 4n\\sigma^2) \\quad \\text{where} \\quad \\sigma^2=p(1-p)\n\\] Note that \\(p\\) is assumed to be 0.5 because the coin is fair. But what’s neat is that the assumption of a fair coin is not a requirement for normality to arise. For example, what if the probability of heads is \\(p=0.25\\)?\nThe distribution for \\(n=16\\) is not quite “normal”, in that it is asymetrically skewed towards the right tail (towars the home team’s endzone). However, the normal distribution is not a bad approximation and the central limit theorem states this will still converge given large enough \\(n\\). See for example, \\(n=64\\).\n\nN=16N=64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CLT Revisited\n\n\n\nIn it’s classical form (the form I learned in grad school), the central limit theorem states that for independent samples of \\(X_i\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\), the distribution of the sample mean \\(\\bar{X}\\) converges with large \\(n\\) to a Normal distribution centered at \\(\\mu\\) with predictable variance:\n\\[\n\\bar{X} \\rightarrow N(\\mu, \\frac{\\sigma^2}{n})\n\\]"
  },
  {
    "objectID": "posts/statistical-rethinking/index.html#selecting-priors",
    "href": "posts/statistical-rethinking/index.html#selecting-priors",
    "title": "Rethinking Some Statistical Principles",
    "section": "Selecting Priors",
    "text": "Selecting Priors\nWhile classical inference is often concerned with selecting the best estimators, Bayesian inference depends heavily on the choice of priors. The latter is not something that’s emphasized in most courses (indeed, not in my graduate school classes), and it’s usually assumed that priors emerge out of a textbook when needed to solve a Bayesian problem, or that they are extremely vague and uninformative.\nWhat I like about this chapter is its insistence on selecting appropriate priors, and then checking the assumptions reinforced by priors before analyzing any data. This is a sensible thing to do; indeed the choice of different priors may distinguish two anlysts from one another, but to even begin analyzing data one ought to reassure themselves that the prior chosen is at least appropriate.\nConsider the example from this chapter, which analyzes heights as Gaussian-distributed. In a way that’s surprisingly common in other textbooks, one might suppose that the prior on the mean height \\(\\mu\\) is uninformative, for example:\n\\[\n\\begin{align}\nh_i \\sim \\text{Normal}(\\mu, \\sigma^2) \\\\\n\\mu \\sim \\text{Normal}(0, 100^2) \\\\\n\\sigma \\sim \\text{Unif}(0, 50) \\\\\n\\end{align}\n\\] The choice of prior for \\(\\mu\\) is uninformative, yes, but it’s actually a preposterous assumption. Human heights are not distributed with such high variance, nor are they centered at zero. But this is exactly the kind of choice that a naive observer would want to make if they knew nothing about human heights. Instead the author posits a model that is reasonable given his height of 178cm:\n\\[\n\\begin{align}\nh_i \\sim \\text{Normal}(\\mu, \\sigma^2) \\\\\n\\mu \\sim \\text{Normal}(178, 20^2) \\\\\n\\sigma \\sim \\text{Unif}(0, 50) \\\\\n\\end{align}\n\\]\nBelow I compare the two choices of priors through a prior predictive check:\n\nUninformativeInformative\n\n\n\n\nCode\nsample.mu &lt;- rnorm(10000, mean=0, sd=100)\nsample.sigma &lt;- runif(10000, min=0, max=50)\nprior.h &lt;- rnorm(10000, mean=sample.mu, sd=sample.sigma)\nhist(prior.h, main=NA, xlim=c(-400, 400))\nmtext(\"Prior on Height (cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsample.mu &lt;- rnorm(10000, mean=178, sd=20)\nsample.sigma &lt;- runif(10000, min=0, max=50)\nprior.h &lt;- rnorm(10000, mean=sample.mu, sd=sample.sigma)\nhist(prior.h, main=NA, xlim=c(-400, 400))\nmtext(\"Prior on Height (cm)\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe informative prior might seem like “cheating” in some respect because it is more precise to begin with, but such precision is the exact reason we’d like to work with priors in the first place. Priors help to constrain our estimates to ranges we believe plausible. The alternatives–keeping priors vague or reverting to classical methods–might be lauded as “objective” but fail to provide meaningful insight when our sample size is limited (as it almost always is). Even worse, when folks see an objective result that doesn’t agree with their intuition, they come up with subjective reasons to discount it, and those reasons are almost never formalized with the same transparency as a prior selection."
  },
  {
    "objectID": "posts/statistical-rethinking/index.html#footnotes",
    "href": "posts/statistical-rethinking/index.html#footnotes",
    "title": "Rethinking Some Statistical Principles",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStatistical Rethinking by Richard McElreath. Second Edition. https://xcelab.net/rm/.↩︎\nEfficiency Property of Maximum Likelihood Estimators. https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Efficiency.↩︎\nRethinking package for R. https://github.com/rmcelreath/rethinking.↩︎\nMetropolis-Hastings Algorithm. https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm.↩︎"
  },
  {
    "objectID": "posts/pitcher-survival/index.html",
    "href": "posts/pitcher-survival/index.html",
    "title": "Starting Pitcher Replacements in the Postseason: Time-to-event methods",
    "section": "",
    "text": "Recently, starting pitchers in baseball have been pulled at earlier points in the game, especially in high leverage moments. This trend is particularly noticeable in the postseason, where managers are tempted to pull their starters in favor of a reliever to get out of a defensive jam. In this post I will use survival analysis to study the timing of starting pitcher replacements in the postseason. I will also introduce the concepts of censoring and survival curves as they relate to starting pitcher replacements. Ultimately, I aim to build upon this post and develop a statistical model for predicting starting pitcher replacements."
  },
  {
    "objectID": "posts/pitcher-survival/index.html#survival-analysis-a-brief-overview",
    "href": "posts/pitcher-survival/index.html#survival-analysis-a-brief-overview",
    "title": "Starting Pitcher Replacements in the Postseason: Time-to-event methods",
    "section": "Survival Analysis: A brief overview",
    "text": "Survival Analysis: A brief overview\nThe goal of survival analysis is to understand the factors that lead to earlier (or later) event times. In a medical context, the event is often mortality (i.e., death of the patient). In this sense, medical researchers are typically most-concerned with the factors that lead to earlier mortality, or longer survival, hence the name ‘survival’ analysis.\n\n\n\nFigure 1: The Kaplan-Meier method illustrates the survival probabilities of two cohorts in a randomized trial (treatment and control). The NCSS software website provided this figure, along with a helpful overview of survival analysis. Source: https://www.ncss.com/software/ncss/survival-analysis-in-ncss/\n\n\nSurvival analysis is particularly useful when studying time-related phenomena with many possible risk factors, and when observational data are available. In professional sports, risk factors might be understood as game situations, i.e., what situations lead to a higher chance of the outcome? Data availability is usually not an issue, since events are now tracked to the play-level, and sometimes even greater levels of granularity.\nThere aren’t many examples of survival analysis applied to baseball, though one example studied how the length of player careers varied based upon the handedness of the player.1 In that analysis, the event of interest was the end of the player’s career; the unit of time was days. This analysis could help answer questions like, which throwing handedness tends to yield longer careers in the major leagues? (Spoiler: the author found, on average, right-handed throwers teneded to last a little longer than lefties).\nSurvival analysis includes a lot of the same features of linear regression. Linear regression is at the foundation of statistical learning. It is used in advanced baseball metrics to adjust for factors like league-to-leage variability or positional differences.2 Importantly, survival analysis extends that framework to study events that are time-dependent, like the length of a player’s career. Studying career lengths with linear regression would be prone to certain kinds of bias, since career lengths are not normally-distributed.3\nIn the next section I will describe the data that were prepared to analyze starting pitcher replacements in the postseason."
  },
  {
    "objectID": "posts/pitcher-survival/index.html#data-play-by-play-from-retrosheet",
    "href": "posts/pitcher-survival/index.html#data-play-by-play-from-retrosheet",
    "title": "Starting Pitcher Replacements in the Postseason: Time-to-event methods",
    "section": "Data: Play-by-play from Retrosheet",
    "text": "Data: Play-by-play from Retrosheet\nFor this analysis, I used play-by-play data from Retrosheet4 to construct a set of starting pitcher histories in the postseason. This required parsing the play-by-play events to identify game situations, such as the number of runners on base. It also required processing roster information to identify the handedness of the pitchers and batters they faced. In fact this data cleaning process was intensive, since the play-by-play data are minimally-structured and contain only enough information to re-create the context of a particular game after applying a considerable amount of processing. Documentation for the play-by-play files is available5 and was extremely helpful in this stage of the analysis.\nI chose to focus on the twenty postseasons between 2000 and 2019. This was for a few reasons. First, the more-recent postseasons since 2019 have been impacted by COVID-19 and major league rule changes, such as the pitch clock. I wanted to ensure my analysis was not complicated by any of those effects. Secondly, the postseasons before 2000 are now so long ago that strategies around managing a starting pitcher are substantially different. Thus, I focus on 2000-2019 as a relatively stable, modern era of postseason play. Within this window there were 611 postseason games available to analyze.\nHere’s what every postseason game log contains, after data processing was performed:\n\nLeague (e.g., NL/AL/World Series)\nPlayoff round\nPlayoff game number\nThe starting pitcher for each team\nA record of all plate appearances facing each starter, and outcomes (e.g., home-run, fly out)\nA running tally of the number of pitches thrown by each starter\nA running tally of the number of strikeouts achieved by each starter\nThe handedness of the pitcher and batter\n\nThe above list is incomplete since it does not contain crucial variables like the matchup history between a given batter and pitcher. However, I felt that the list was sufficient for beginning an inquiry into starting pitchers and the factors that affect their replacement. In the next few sections, I will define the key variables of interest for the analysis.\n\n\n\nFigure 2: An example of the game log for the 2019 NLCS (STL vs. WSH, game 4). Patrick Corbin was on the mound for the Nationals when Yadier Molina solo homer’d in the fourth inning. The Nats went on to win 7-4 behind Corbin’s 12-strikeout start."
  },
  {
    "objectID": "posts/pitcher-survival/index.html#defining-the-event-of-interest",
    "href": "posts/pitcher-survival/index.html#defining-the-event-of-interest",
    "title": "Starting Pitcher Replacements in the Postseason: Time-to-event methods",
    "section": "Defining the Event of Interest",
    "text": "Defining the Event of Interest\nIn the playoffs, the time of replacing the starting pitcher is one of the most crucial decisons faced by managers. Here is what sportswriter Jasyon Stark wrote about “hooks” in the wake of a catastrophic collapse of the Red Sox’s pitching in game seven of the 2003 ALCS:\n\nFor managers and the pitchers they manage, this is the question: Take him out, or leave him in? It is the question that hangs over every manager every game of every season. It is the question that leaves canyons under their eyes and fault lines in their forehead.6\n\nRed Sox manager Grady Little had just been fired after the team ended their season on a blown lead against the Yankees. Little’s decision to leave Pedro Martinez on the mound in the eighth inning was the turning point that contributed to their loss.\n\n\n\nFigure 3: Kevin Cash’s decision to pull Blake Snell in game six of the 2020 World Series ultimately backfired. Managers face consequential decisions like this throughout the playoffs, especially in high leverage situations. Photo: Tim Heitman (USA Today).\n\n\nIn the modern era, complete games are increasingly rare. In the dataset described in the previous section, of the 1218 starts analyzed, only 33 (2.7%) resulted in complete games. Thus managers are usually grappling with when to pull the starter, not if they should pull the starter. Of course, in the heat of the moment of a playoff game, this decision can resemble the form of a binary choice for each batter faced. In that sense, it’s like flipping a coin at the beginning of each plate appearance. The manager keeps flipping the coin (with a low probabilitiy of tails), and it continues to show heads. However flip the coin enough times and eventually it shows tails. When this happens, the starter gets pulled.\nI’m not suggesting that managers are actually flipping coins to guide them through highly consequential decisions, but to an outside observer, the process of starting pitcher replacements can be thought of as a random process. In this analysis, I will define the random process as the replacement of the starter, and I will model the timing of that event using information available throughout the game, including the in-game performance of the pitcher and the opposing team’s offense. Next I will define the unit of time used to measure how late into the game a starter will play."
  },
  {
    "objectID": "posts/pitcher-survival/index.html#defining-the-unit-of-time",
    "href": "posts/pitcher-survival/index.html#defining-the-unit-of-time",
    "title": "Starting Pitcher Replacements in the Postseason: Time-to-event methods",
    "section": "Defining the Unit of Time",
    "text": "Defining the Unit of Time\nIn most survival analyses, the unit of time is apparent from the context. For example, in a study of biomedical treatments for cancer, it would make sense to measure time in years, since patients would be expected to survive for multiple years after receiving treatment. In the previously mentioned analysis of players’ career lengths, time was measured in days. Whether days or years are used doesn’t really matter. The point is that time was measured with clock time.\nContrast this with starting pitcher performances, where it doesn’t make sense to measure time in minutes or hours, because baseball is not a sport with a running clock. What is the appropriate choice for a unit of time in measuring how long a starter lasts? Innings pitched? Batters faced? Pitches thrown? The answer is not so obvious, and the choice time units requires some careful thinking.\nConsider that most starting pitchers are evaluated on innings pitched. You’d like to see a workhorse starter throw for more innings, on average. But in baseball, there is no uniform length of an inning. In fact, two innings may last very different lengths of time, determined by relative offensive and defensive performance. For that reason, innings pitched is not the most appropriate unit of time for this analysis.\nInstead of innings pitched, the number of batters faced is a relevant proxy for time. Pitchers may throw different numbers of pitches, but you can generally expect a pitcher to throw about 3.8 pitches per batter faced in the long run. Additionally, pitchers are often replaced before facing a new batter, not mid-way through an at-bat.7 Lastly, the number of batters faced can easily be converted to times through the order (TTO); simply divide the number of batters by nine. Thus the number of batters faced is a sensible choice for the unit of time.\nThe following plot shows the distribution of the number of total batters faced (TBF) for all of the playoff starters in the dataset. The small number of complete games is illustrated by the tightly-grouped cluster at the right end of the distribution. Overall, starters tended to face between 20 and 30 batters, and rarely faced more than 35 batters. However, there was a substantial amount of starters that were pulled early, i.e., between 10 and 20 batters faced.\n\n\n\nFigure 4: Number of batters faced by all starters in the 2000-2019 postseasons. There were 1,218 starts, of which 33 resulted in complete games (2.7%)."
  },
  {
    "objectID": "posts/pitcher-survival/index.html#censoring-handling-complete-games",
    "href": "posts/pitcher-survival/index.html#censoring-handling-complete-games",
    "title": "Starting Pitcher Replacements in the Postseason: Time-to-event methods",
    "section": "Censoring: Handling complete games",
    "text": "Censoring: Handling complete games\nWhen using survival analysis methods, one has to consider the possibility that the event does not occur within the observation window. In medical contexts, this might be good news for the patient; fewer events mean fewer deaths. In this analysis, the replacement of a starting pitcher in the postseason is extremely common. The small number of complete games means we could probably ignore them altogether without much of an effect on the results.\nHowever, what if we wanted to include complete games in our analysis anyways? The methods of survival analysis are well-equipped to handle these observations. In the context of this analysis, complete games are considered censored observations, because the game ended before the starting pitcher was replaced. Survival analysis will not ignore the censored observations, it turns out, and instead each model will use data from complete games to inform an understanding of what makes a starter more likely to get pulled.\nI should mention, the implicit assumption of censoring is that the event would have occurred if we were to continue observing the phenomenon into perpetuity. In baseball, this assumption is somewhat questionable since complete games imply that the starter was not replaced. But imagine if the opposing team were to tie the game and send it into extra innings. Eventually, it would become more likely that the starter would be replaced, even if he is pitching very well, because of fatigue and injury risk."
  },
  {
    "objectID": "posts/pitcher-survival/index.html#kaplan-meier-method-for-replacing-starters",
    "href": "posts/pitcher-survival/index.html#kaplan-meier-method-for-replacing-starters",
    "title": "Starting Pitcher Replacements in the Postseason: Time-to-event methods",
    "section": "Kaplan-Meier Method for Replacing Starters",
    "text": "Kaplan-Meier Method for Replacing Starters\nThe Kaplan-Meier procedure8 helps to estimate the speed of a ‘survival’ process, like the process of replacing starting pitchers described earlier in this post. The Kaplan-Meier method is usually illustrated with one or more ‘curves’. The curves in this case are functions that represent estimated probabilities of survival at various stages in the process. The higher the estimated probability, the more likely one is to survive until that particular point in time.\nBefore jumping to the Kaplan-Meier curves for this dataset, it’s worth considering which variable to stratify; there are a number of choices including league, series, and pitcher handedness. The point of stratification is to examine whether survival patterns vary between different strata of the overall dataset. In the figure below I stratified by season, but grouped seasons into five-year eras starting with 2000-2004. Therefore there are a total of four strata: 2000-04, 2005-09, 2010-14, and 2015-19.\nThe following plot shows the Kaplan-Meier curves for the probability of replacing a starting pitcher in the postseason, stratified by season era. The curves are plotted against the number of batters faced, which is the unit of time that was chosen earlier. The curves begin at zero on the \\(x\\)-axis, because it is impossible for a starter to be replaced before the first batter. The curves end at about 40, because that was the highest number of total batters faced in any postseason game during this period.\n\n\n\nFigure 5: Kaplan-Meier curves illustrating the probability of remaining in a postseason game, for starters in the 2000-2019 era. Curves are stratified by 5-year windows. The unit of time is the number of total batters faced (TBF).\n\n\nThe most obvious pattern in the above picture is that the purple curve, representing 2015-2019, is shifted to the left of the other three curves. This era is the latest in the dataset, and the shifted curve suggests that starters tended to be replaced earlier in the game during the last five seasons. Another cue is taken from the point where the curves cross a probability of 50% (when the \\(y\\)-axis reads 0.5); this is the estimated median survival time. For the season eras between 2000 and 2014, the median number of batters faced was 26. For the 2015-19, however, the median TBF was 23. That suggests pitchers were pulled about three batters earlier in the game during the latter era, compared with the earlier era.\nIs a difference of three batters meaningful? I’d argue yes, since a lot can happen in in just one or two plate appearances, particularly in high-leverage moments with the game on the line. But another important question is whether the difference is statistically significant. The Kaplan-Meier method answers that question, too. I applied the log-rank test which assesses whether there is a difference between the four curves. The \\(p\\)-value of the test was very small (&lt; 0.001), meaning the difference is statistically significant.\nAlthough this preliminary analysis suggests that starting pitchers did not last as long during the 2015-19 era as they did in previous eras, there are additional factors that we might consider relevant. For example, did something else change in the style of play that made managers more likely to pull their starters early? In a future post I will address the possibility of confounding factors using a statistical method called Cox Proportional Hazards regression."
  },
  {
    "objectID": "posts/pitcher-survival/index.html#footnotes",
    "href": "posts/pitcher-survival/index.html#footnotes",
    "title": "Starting Pitcher Replacements in the Postseason: Time-to-event methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMicah Melling. “Survival Analysis: How Long Do Careers Last?.” Baseball Data Science. https://www.baseballdatascience.com/survival-analysis-how-long-do-careers-last/↩︎\nPiper Slowinski. “What is WAR?”. FanGraphs. https://library.fangraphs.com/misc/war/↩︎\nWikipedia. Normal Distribution. https://en.wikipedia.org/wiki/Normal_distribution↩︎\nRetrosheet. Play-by-Play Data Files: Post-Season Event Files. https://www.retrosheet.org/game.htm#Post-Season%20Games↩︎\nRetrosheet. The Event File. https://www.retrosheet.org/eventfile.htm↩︎\nJayson Stark. “The book on hooks.” ESPN.com: Baseball. https://www.espn.com/mlb/columns/story?columnist=stark_jayson&id=1798057↩︎\nAlden Woods. “Girardi makes pitching change mid-at-bat.” MLB.com. https://www.mlb.com/news/joe-girardi-makes-pitching-change-mid-at-bat/c-141252666↩︎\nE.L. Kaplan & Paul Meier. “Nonparametric Estimation from Incomplete Observations.” Journal of the American Statistical Association. https://doi.org/10.1080/01621459.1958.10501452↩︎"
  },
  {
    "objectID": "adventure.html",
    "href": "adventure.html",
    "title": "Adventures",
    "section": "",
    "text": "Outside of my job as a data scientist, I am an avid rock & alpine climber. I’ve climbed in 12 US states as well as in Mexico, tallying over 1,000 pitches since I began outdoor climbing. Below, I’ve compiled a list of my best trip reports below, some of which are unpublished.\n\n  \n  \n    \n    \n  \n  \n  \n  \n    \n    A day-long adventure in Colorado's most famous national park. We encountered a steep snow field on our approach to the Sharkstooth Ridge.\n    \n  \n  \n    Partner: Jason K.\n    Route: Northeast Ridge (5.6, II)\n  \n  \n    Link to Full Trip Report\n  \n  \n    Published 02/08/2024\n  \n\n\n  \n  \n    \n    \n  \n  \n    \n    My first real mountaineering trip, in which I climbed the Grand Teton using a classic rock climbing route to the summit.\n    \n  \n  \n    Partner: Donnie K.\n    Route: Exum Ridge (5.5, III)\n  \n  \n    Link to Draft Trip Report\n  \n  \n    Unpublished\n  \n\n\n  \n  \n    \n    \n  \n  \n    \n    An outing with the Explorer's Club of Pittsburgh. We hiked through the night to reach our rock climbing objective at Seneca Rocks, WV. We summitted and bivied atop the rocks.\n    \n  \n  \n    Partners: Evren G., Dan K.\n    Route: Old Man's (5.3)\n  \n  \n    Link to Draft Trip Report\n  \n  \n    Unpublished"
  },
  {
    "objectID": "adventure.html#trip-reports",
    "href": "adventure.html#trip-reports",
    "title": "Adventures",
    "section": "",
    "text": "Outside of my job as a data scientist, I am an avid rock & alpine climber. I’ve climbed in 12 US states as well as in Mexico, tallying over 1,000 pitches since I began outdoor climbing. Below, I’ve compiled a list of my best trip reports below, some of which are unpublished.\n\n  \n  \n    \n    \n  \n  \n  \n  \n    \n    A day-long adventure in Colorado's most famous national park. We encountered a steep snow field on our approach to the Sharkstooth Ridge.\n    \n  \n  \n    Partner: Jason K.\n    Route: Northeast Ridge (5.6, II)\n  \n  \n    Link to Full Trip Report\n  \n  \n    Published 02/08/2024\n  \n\n\n  \n  \n    \n    \n  \n  \n    \n    My first real mountaineering trip, in which I climbed the Grand Teton using a classic rock climbing route to the summit.\n    \n  \n  \n    Partner: Donnie K.\n    Route: Exum Ridge (5.5, III)\n  \n  \n    Link to Draft Trip Report\n  \n  \n    Unpublished\n  \n\n\n  \n  \n    \n    \n  \n  \n    \n    An outing with the Explorer's Club of Pittsburgh. We hiked through the night to reach our rock climbing objective at Seneca Rocks, WV. We summitted and bivied atop the rocks.\n    \n  \n  \n    Partners: Evren G., Dan K.\n    Route: Old Man's (5.3)\n  \n  \n    Link to Draft Trip Report\n  \n  \n    Unpublished"
  },
  {
    "objectID": "adventure.html#photography",
    "href": "adventure.html#photography",
    "title": "Adventures",
    "section": "Photography",
    "text": "Photography\nI am beginning to share some of my photos from rock climbing adventures.\nFall in West Virginia, 2024"
  },
  {
    "objectID": "posts/climbing-pyramids/index.html",
    "href": "posts/climbing-pyramids/index.html",
    "title": "A Tool to Visualize Outdoor Climbing Performance",
    "section": "",
    "text": "Recently, I’ve been interested in developing ways to assess my own performance as an outdoor rock climber. One classic tool for evaluating a climber’s history on routes is called the ‘pyramid’ (also known as the ‘scorecard’ on other sites). In this post I discuss how I built a simple web app to illustrate my own climbing pyramid as well as that of any other climber who uses MountainProject.com to track their outdoor climbing performance."
  },
  {
    "objectID": "posts/climbing-pyramids/index.html#motivation",
    "href": "posts/climbing-pyramids/index.html#motivation",
    "title": "A Tool to Visualize Outdoor Climbing Performance",
    "section": "Motivation",
    "text": "Motivation\nI’ve been using Mountain Project meticulously since I began outdoor rock climbing in 2019. On most of my climbing trips, I log ascents in the iPhone app, which syncs with my profile online. This has generally been something I treated as a personal journalling exercise, and I figured I would someday want to look back on my ascents to see how far I’ve come in the sport/adventure of rock climbing.\nRecently, I’ve been focused on physical training to improve my climbing performance outside, and the data that I’ve gathered over the past five years would seem to be an invaluable asset. It turns out that I’m not alone, and many other climbers use the same features on Mountain Project to log their ascents, also known as ‘ticks’ on the site\n\n\n\nThe publicly-available ticks for a route called ‘Scenic Adult’ (5.11c) at the New River Gorge in West Virginia, as of this post (November 2024)\n\n\nThere remains a challenge in leveraging this data, though, which is that metrics of historical climbing performance are not very quantifiable – and those that are quantitative are generally not sophisticated, like Max Redpoint Grade, or Hardest Flash. Those who spend a lot of time in climbing gyms will be familiar with these ideas. In essence, these metrics are akin to asking a runner what their best marathon time is, which can tell you a lot about someone’s running ability. However, it tells you less about how they’ve progressed over time and how they’ve spent their effort to arrive at their current performance level. These questions are more nuanced, and in climbing are arguably more important than just knowing someone’s maximum performance."
  },
  {
    "objectID": "posts/climbing-pyramids/index.html#climbing-pyramids",
    "href": "posts/climbing-pyramids/index.html#climbing-pyramids",
    "title": "A Tool to Visualize Outdoor Climbing Performance",
    "section": "Climbing Pyramids",
    "text": "Climbing Pyramids\nOne tool that might help climbers address some of the above questions is called the pyramid. The pyrmaid plots the number of successful (and sometimes unsuccessful) ascents of climbs at a particular grade. Without going into too much detail, rock climbs are graded very rigorously1, a little bit like the green/blue/black grading system of ski routes. While being subject to a fair bit of subjectivity, the grade of a rock climb is usually a reliable measure of how difficult that route will be for the average person.\nThe pyramid tool can help visualize how a climber devotes her finite resources (e.g., time) to climbing routes of a particular grade range. It is a snapshot in time of how well-balanced a climber’s performance is, with a ‘top-heavy’ shape suggesting a climber focuses most of his time on difficult routes, and a ‘bottom-heavy’ shape suggesting he should probably try more difficult routes. There’s a lot of insight to be gained by seeing your own pyramid, and to illustrate an example here is my pyramid as of this writing:\n\n\n\nMy climbing pyramid, as of November 2024, with ‘successful’ ascents highlighted in red and failed attempts highlighted in blue.\n\n\nI am no expert but I have an intuition that my climbing pyramid is fairly typical of people at my ability level, with a few unique quirks such as the fact that I’ve spent a lot of time climbing routes at the grade of 5.11a. Additionally, while my max redpoint grade is currently 5.12a, I still have room to reinforce my experience at the grades just below that, namely 5.11c and 5.11d. These are just a few of the takeaways from quickly analyzing my own pyramid."
  },
  {
    "objectID": "posts/climbing-pyramids/index.html#the-web-app",
    "href": "posts/climbing-pyramids/index.html#the-web-app",
    "title": "A Tool to Visualize Outdoor Climbing Performance",
    "section": "The Web App",
    "text": "The Web App\nUsing the very handy Shiny2 package for R, I developed a web application called the ‘Tick Pyramid Tool’ (subject to name change), which enables Mountain Project users to easily visualize their own climbing pyramids, just as I showed above. I also added the ability to view your progression over time, which simply tracks how your max grade has increased (hopefully by a lot!) since you started climbing.\n\n\n\nScreenshot of the current web app, which is hosted by Shiny Apps at rpomponio.shinyapps.io/climbing_viz_app\n\n\nThis is an active work-in-progress and I plan to add more features to the web app as I solicit feedback from other climbers. However, I figured it is worth highlighting some of the key features in this ‘beta’ version:\n\nUsers can upload their own ticks (from MountainProject.com) or paste their profile URL.\nSee a complete list of all the climbs you’ve logged, and apply filters for route type, dates, and whether you want to include repeated ascents of the same route.\nVisualize your route pyramid (click the second tab across the top, or scroll down on mobile).\nVisualize your climbing progression over time.\n\nIf you are an R programmer and would like to contribute to the development of this tool, then I welcome you to fork the development branch on GitHub3."
  },
  {
    "objectID": "posts/climbing-pyramids/index.html#footnotes",
    "href": "posts/climbing-pyramids/index.html#footnotes",
    "title": "A Tool to Visualize Outdoor Climbing Performance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Grade_(climbing)↩︎\nShiny: Web Application Framework for R. https://cran.r-project.org/package=shiny↩︎\nhttps://github.com/rpomponio/tick_pyramid_tool/tree/dev↩︎"
  },
  {
    "objectID": "posts/statistical-rethinking/index.html#testable-implications",
    "href": "posts/statistical-rethinking/index.html#testable-implications",
    "title": "Rethinking Some Statistical Principles",
    "section": "Testable Implications",
    "text": "Testable Implications\nThis chapter, which emphasizes multiple regression techniques, begins with a surprising discourse on causal inference. I’m thankful for the opportunity to review some causal concepts, since just like Bayesian methods, causal inference is not often taught in traditional statistics curricula.\nHere are some directed acyclic graphs (DAGs) that were proposed, in an attempt to explain the phenomenon of variability in state-level divorce rates:\n\nCode\ndag.mediator &lt;- dagitty( \"dag {\n    Age -&gt; Divorce\n    Age -&gt; Marriage\n    Marriage -&gt; Divorce}\")\ncoordinates(dag.mediator) &lt;- list(\n  x=c(Age=0, Divorce=1, Marriage=2),\n  y=c(Age=0, Divorce=1, Marriage=0))\nplot(dag.mediator)\n\ndag.independ &lt;- dagitty( \"dag {\n    Age -&gt; Divorce\n    Age -&gt; Marriage}\")\ncoordinates(dag.independ) &lt;- list(\n  x=c(Age=0, Divorce=1, Marriage=2),\n  y=c(Age=0, Divorce=1, Marriage=0))\nplot(dag.independ)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Effect of age (at marriage) on divorce rate mediated by marriage rate.\n\n\n\n\n\n\n\n\n\n\n\n(b) Effect of marriage rate is conditionally independent of divorce rate.\n\n\n\n\n\n\n\nFigure 1: DAGs from Chapter 5\n\n\n\nNow the testable implication of Figure 1 is whether the marriage rate is conditionally independent of divorce. In formal terms:\n\\[\n\\begin{align}\nM \\perp D |A \\\\\n\\quad \\text{where} \\quad M:\\text{Marriage rate,} \\quad D:\\text{Divorce rate,} \\\\\n\\quad \\text{and} \\quad A:\\text{Age at marriage (median)}\n\\end{align}\n\\] This example introduced new intuition that I had not previously encountered in my graduate statistics training. One, the idea of conditional independence is not new, but knowing that it could be gleaned from a DAG is a valuable lesson. It highlights the value in drawing these diagrams before modeling. Two, the concept of mediators is not new either, but knowing that a mediator model implies all three variables are associated with one another is a good reminder of the differences between data structures and causal implications. We might observe many correlations within an observed set of data, some of them spurious. However, having a DAG to return to makes it easier to theorize how such correlations came to be.\nHere’s a simulated example of conditional independence, in which marriage rate is conditionally independent of divorce rate, yet a linear regression still picks up on the strong (unadjusted) relationship between the two:\n\n\nCode\nset.seed(1818)\nN &lt;- 50\nage &lt;- rnorm(N)\nmar.rate &lt;- rnorm(N, -age)\ndiv.rate &lt;- rnorm(N, age)\nprecis(lm(div.rate ~ mar.rate), prob=0.95)\n\n\n                   mean        sd       2.5%      97.5%\n(Intercept) -0.07556679 0.1834771 -0.4351754  0.2840418\nmar.rate    -0.54643398 0.1118293 -0.7656153 -0.3272527\n\n\nWithout adjusting for age, we would mistakenly conclude that marriage rate is the solely important predictor of divorce rate (with a confidence interval that includes negative boundaries). In reality, this association disappears when we “control” for age:\nTerminology: The book makes a compelling argument against use of the word “control”, which is common in statistical parlance, because it suggests more power than is truly available. In lieu of an experimental control, the best we can do is adjust for predictors that might share a causal relationship with the outcome. That is what’s accomplished in multiple regression.\n\n\nCode\nprecis(lm(div.rate ~ mar.rate + age), prob=0.95)\n\n\n                  mean        sd        2.5%      97.5%\n(Intercept) -0.1754270 0.1130081 -0.39691877 0.04606467\nmar.rate     0.1950782 0.1072680 -0.01516319 0.40531955\nage          1.4164137 0.1576099  1.10750397 1.72532335"
  },
  {
    "objectID": "posts/statistical-rethinking/index.html#counterfactual-plots",
    "href": "posts/statistical-rethinking/index.html#counterfactual-plots",
    "title": "Rethinking Some Statistical Principles",
    "section": "Counterfactual Plots",
    "text": "Counterfactual Plots\nOf the various diagnostics for multiple regression, I found the proposition of a counterfactual plot most compelling. They are constructed as follows:\n\nChoose the predictor (intervention) to manipulate.\nDefine the range of values for the chosen predictor.\nLeverage the posterior to simulate values of other variables, including both the outcome and dependent predictors.\n\nIt’s the third step that’s new for me. I am not used to considering the intermediate effects of a change in the intervention, but that seems to be exactly what Bayesian models are equipped to handle. Here’s the textbook example:\n\n\nCode\ndata(\"WaffleDivorce\")\nstates &lt;- list(\n  A = standardize(WaffleDivorce$MedianAgeMarriage),\n  M = standardize(WaffleDivorce$Marriage),\n  D = standardize(WaffleDivorce$Divorce))\nfit.counterfac &lt;- quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu &lt;- alpha + beta_mar * M + beta_age * A,\n    alpha ~ dnorm(0, 0.2),\n    beta_mar ~ dnorm(0, 0.5),\n    beta_age ~ dnorm(0, 0.5),\n    sigma ~ dexp(1),\n    M ~ dnorm(theta, gamma), ### I'm using theta, gamma for the mediator model\n    theta &lt;- eta + delta_age * A,\n    eta ~ dnorm(0, 0.2),\n    delta_age ~ dnorm(0, 0.5),\n    gamma ~ dexp(1)),\n  data=states)\nprecis(fit.counterfac)\n\n\n                   mean         sd       5.5%      94.5%\nalpha     -1.866974e-05 0.09707569 -0.1551644  0.1551270\nbeta_mar  -6.541709e-02 0.15077205 -0.3063799  0.1755458\nbeta_age  -6.134788e-01 0.15098351 -0.8547796 -0.3721780\nsigma      7.851144e-01 0.07784255  0.6607069  0.9095218\neta        2.173672e-05 0.08684748 -0.1387773  0.1388208\ndelta_age -6.947480e-01 0.09572636 -0.8477372 -0.5417588\ngamma      6.817335e-01 0.06757919  0.5737289  0.7897381\n\n\nFrom this model, it’s clear that age and marriage rate are associated through the posterior summary of \\(\\delta_{\\text{Age}}\\), which is consistently negative. Therefore the effect of an increase in the median age of marriage is associated with a decrease in the marriage rate, which makes perfect sense.\nNext I carry out the three steps necessary to create a counterfactual portrait of a change in the median age at marriage, all in the following code block:\n\nage.seq &lt;- seq(-2, 2, length.out=10)\npost &lt;- extract.samples(fit.counterfac)\nsim.counterfac &lt;- list()\n# simulate a 1000 x 10 matrix of marriage rates...\nsim.counterfac$M &lt;- sapply(\n  age.seq,\n  function(age) rnorm(1000, post$eta + post$delta_age * age, post$gamma))\n# next, simulate a 1000 x 10 matrix of divorce rates, conditional on prev. sim\nsim.counterfac$D &lt;- sapply(\n  1:length(age.seq),\n  function(i) rnorm(\n    1000,\n    post$alpha + post$beta_age * age.seq[i] + post$beta_mar * sim.counterfac$M[, i],\n    post$gamma))\n\nI think the trick here is to use the simulated values of the marriage rate (conditional on chosen ages) to simulate–in-turn–values of the divorce rate. I can say confidently that I would not have though to do this, especially within the scope of one model. But the posterior makes this trivial to examine:\n\nCode\nplot(age.seq, colMeans(sim.counterfac$D), ylim=c(-2, 2), type=\"l\",\n     xlab=\"Manipulated Age\", ylab=\"Counterfactual Divorce\")\nshade(apply(sim.counterfac$D, 2, PI), age.seq)\n\nplot(age.seq, colMeans(sim.counterfac$M), ylim=c(-2, 2), type=\"l\",\n     xlab=\"Manipulated Age\", ylab=\"Counterfactual Marriage\")\nshade(apply(sim.counterfac$M, 2, PI), age.seq)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Effect of age (at marriage) on divorce rate, the outcome.\n\n\n\n\n\n\n\n\n\n\n\n(b) Effect of age (at marriage) on marriage rate, the mediator.\n\n\n\n\n\n\n\nFigure 2: Counterfactual Plots from Chapter 5\n\n\n\nThe plot in Figure 2 illustrates the entire strength of associations illustrated earlier in Figure 1; namely, that age is associated with both marriage rate and divorce rate. Thus, intervening to alter the age of marriage will cascade downstream to effect both the mediator and the outcome, where the outcome is doubly affected by both the intervention and the mediator."
  }
]