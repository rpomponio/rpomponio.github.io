---
title: "Rethinking Some Statistical Principles"
author: "Ray Pomponio"
date: today
categories: [reading, reflections]
---

```{r}
#| echo: false
#| message: false
#| warning: false
library(rethinking)
library(dagitty)
library(splines)
```

I've begun reading *Statistical Rethinking* by Richard McElreath,[^1] and I want to log some of my thoughts that came to mind while working through his ideas on modern statistical science.

# Chapter 2 (Modes of Inference)

## Likelihoods

In this chapter, a hypothetical experiment was introduced in which an observer collects data about the proportion of water on a globe (to understand the true proportion of water on earth).

Let $p$ denote the proportion of water on the globe, $W$ the number of observations that yielded "water", and $L$ the number of observations that yielded "land". By the Binomial distribution: 

$$
Pr(W=w,L=l\quad|\quad p)={w+l\choose w}p^w(1-p)^l
$$

Now presume the observer recorded $W=6$ and $L=3$ in nine trials. The "likelihood" of these data can be plotted as a function of probability:

```{r}
#| echo: false
curve(dbinom(6, size=9, prob=x), from=0, to=1, ylab="Likelihood", xlab="p")
```

And this is **by no means a probability distribution**. It doesn't sum to one; it integrates to `r integrate(function(p) dbinom(6, size=9, prob=p), lower=0, upper=1)$value`. However the recognition of this function is a crucial step towards two possible modes of inference:

* In a **classical** sense, this function represents an objective to maximize, then compare against a null hypothesis (often $H_0: p=0.5$).
* In a **Bayesian** sense, this function represents one piece of the joint distribution of $W, L, p$ (the other piece being the prior on $p$). And this is the mode of inference advocated by the book.

When we start with a uniform prior $p\sim\text{Beta}(1, 1)$, we end up with a posterior that looks uncannily like our likelihood function:

:::{.panel-tabset}

### Posterior

```{r}
#| echo: false
curve(dbeta(x, 7, 4), from=0, to=1, ylab="Posterior Density", xlab="p")
```

### Likelihood (again)

```{r}
#| echo: false
curve(dbinom(6, size=9, prob=x), from=0, to=1, ylab="Likelihood", xlab="p")
```

:::

They're actually the same function, just with different *y-axis* scales. I previously didn't realize that...

::: {.callout-tip}
## Likelihoods as Objectives

As part of the classical mode of inference, one might reasonably seek to maximize the above likelihood function $\mathcal{L}$, setting $p=\hat{p}$ at maximal likelihood, then comparing the ratio of likelihoods between $\mathcal{L}(\hat{p})$ and $\mathcal{L}(p_0)$. It turns out, when this ratio is greater than some constant $c$, one would rationally conclude that $p\neq p_0$ (in other words, one rejects the null hypothesis).
:::

So contrasting the two modes of inference: in the classical sense, one compares two points along the likelihood curve to posit evidence against some hypothesis $H_0$. In a Bayesian sense, one obtains a posterior distribution over plausible values of $p$, which can be further analyzed for compatibility with any set of hypotheses. And the realization for me (at least in this trivial example) is that the likelihood and posterior functions are the **exact same shape**.

Reflecting on this a little more, I can understand how the relative simplicity of the classical approach might be appealing at the start of a scientific inquiry, say, when we don't know much about the phenomenon but expect to learn a lot quickly. However, when an inquiry matures to the point of having data readily available, it seems silly to forgo the benefit of having the entire distribution of a parameter available to interrogate. It's clear that the latter offers a richer description of the unknown phenomenon. Perhaps an analogy would be like entering a dark cavernous tunnel, where the classical approach affords you looks at two possible paths forward (one of which is a dead-end and the other quite a promising lead). The Bayesian approach, on the other hand, affords you something like a map of multiple paths forward.

## Algorithms for Posterior Approximation

I was previously aware of posterior approximation using Markov chain Monte Carlo (MCMC), but I was surprised to learn about two viable alternatives to numerically sampling from a posterior distribution.

I'll continue with the hypothetical experiment from the previous section in which we are aiming to estimate $p$, the proportion of water on the globe.

:::{.panel-tabset}

### Grid Approximation

By selecting a finite grid across the support of our parameter $p$, we can approximate the posterior:

```{r}
#| code-fold: true
grid.seq <- seq(0, 1, length.out=20)
prior <- dunif(grid.seq)
likelihood <- dbinom(6, size=9, prob=grid.seq)
posterior <- prior * likelihood / sum(prior * likelihood)
```
```{r}
#| echo: false
plot(grid.seq, posterior, type="b", xlab="p", ylab="Posterior Probability")
mtext("Posterior Approximation with 20 Grid Points")
```

### Quadratic Approximation

This technique involves leveraging a Gaussian approximation of the posterior near its mode. I believe the theory behind it is related to the efficiency property[^2] of MLEs, which states that the MLE converges in distribution to a normal distribution at larger sample sizes. It is relatively easy to code up using the `quap` function from the textbook's `rethinking` package:[^3]

```{r}
#| code-fold: true
posterior <- quap(
  alist(
    W ~ dbinom(W+L ,p) ,
    p ~ dunif(0, 1)),
  data=list(W=6,L=3) )
quad.post <- precis(posterior)
```

```{r}
#| echo: false
curve(dnorm(x, quad.post$mean, quad.post$sd), from=0, to=1, ylab="Quadratic Density", xlab="p")
mtext("Quadratic Approximation to Posterior")
```

### MCMC

Finally MCMC, which is the most-widely adopted approach to posterior approximation, has many algorithmic variants. The following is an example of the Metropolis Algorithm:[^4]

```{r}
#| code-fold: true
set.seed(1818)
n.samples <- 2000
p <- rep(NA, n.samples)
p[1] <- 0.5
obs.data <- list(W=6, L=3)
for (i in 2:n.samples){
  p.new <- rnorm(1 , p[i - 1] , 0.1)
  if (p.new < 0) p.new <- abs(p.new)
  if (p.new > 1) p.new <- 2 - p.new
  q0 <- dbinom(obs.data$W, obs.data$W + obs.data$L, p[i-1])
  q1 <- dbinom(obs.data$W, obs.data$W + obs.data$L, p.new)
  p[i] <- ifelse(runif(1) < q1/q0, p.new, p[i-1])
}
```
```{r}
#| echo: false
hist(p, xlim=c(0, 1), xlab="p", ylab="Posterior Probability", freq=F, main=NA)
mtext("MCMC Approximation to Posterior")
```

:::

The one intriguing feature of the Metropolis Algorithm is the acceptance rule, which computes a ratio of **likelihoods**, given $p$ and $p'$, and accepts the new value $p'$ if this ratio is above some randomly-generated value.

In other words, the internal mechanics of the algorithm work a bit like the classical approach to inference (see above), in which the likelihoods of two points are compared. In the classical approach, $\hat{p}$ is never technically "accepted", but $p_0$ can be rejected if the ratio of likelihoods ($\mathcal{L}(\hat{p})$ vs. $\mathcal{L}(p_0)$) is above some constant $c$. In contrast to MCMC, however, $c$ is not randomly-generate but instead dictated by the choice of $\alpha$, which is commonly 0.05.

Another noteworthy feature, not highlighted here, is that the Metropolis Algorithm does not require computation of exact densities; relative density functions work just fine. That's a major advantage when it comes time to scale the algorithm with increasing model complexity.

```{r}
#| echo: false
#| output: false
# use posterior probabilities from grid approximation
posterior <- prior * likelihood / sum(prior * likelihood)
samples <- sample(grid.seq, prob=posterior, size=9999, replace=TRUE)
hist(samples)
```

# Chapter 4 (Linear Regression)

## Normal (Gaussian) Distributions

This chapter points out the commonality of the Normal Distribution, which serves as a wonderful approximation of natural phenomena such as variation in *height*, variation in *growth rates*, or games of chance. The chapter begins by introducing a hypothetical experiment, which I paraphrase below:

> Suppose 1,000 people line up on the 50-yard line of a American football field. Each person flips a coin 16 times; for each time they see "heads" they step one yard in the direction of the home team's endzone. Likewise, for each time they see "tails" the step one yard towards the away team's endzone.

It's apparent from the simulation below that this somewhat-contrived process will yield a distribution of yardage positions that looks like the following:

```{r}
N <- 16
set.seed(1818)
simulation <- replicate(
  1000,{
  heads <- rbinom(1, size=N, prob=0.5)
  tails <- N - heads
  steps <- heads - tails
  steps
  }
)
barplot(prop.table(table(simulation)), xlab="Position (0=50-yard line)")
mtext(paste("Observed SD:", round(sd(simulation), 3)))
```

By the central limit theorem, it's possible to predict the ultimate distribution of positions. Let $X_i$ be an individual coin flip, and $\bar{X}$ be the mean number of heads for an individual person; there are 1,000 people and $n=16$ tosses per person. The central limit theorem predicts the distribution of $Y=n(2\bar{X}-1)$ will converge towards:

$$
Y \rightarrow N(0, 4n\sigma^2) \quad \text{where} \quad \sigma^2=p(1-p)
$$
Note that $p$ is assumed to be 0.5 because the coin is fair. But what's neat is that the assumption of a fair coin is not a requirement for normality to arise. For example, what if the probability of heads is $p=0.25$?

The distribution for $n=16$ is not quite "normal", in that it is asymetrically skewed towards the right tail (towars the home team's endzone). However, the normal distribution is not a bad approximation and the central limit theorem states this will still converge given large enough $n$. See for example, $n=64$.

:::{.panel-tabset}

### N=16

```{r}
#| echo: false
simulation <- replicate(
  1000,{
  heads <- rbinom(1, size=N, prob=0.25)
  tails <- N - heads
  steps <- heads - tails
  steps
  }
)
barplot(prop.table(table(simulation)), xlab="Position (0=50-yard line)")
mtext(paste("Observed SD:", round(sd(simulation), 3)))
```
### N=64

```{r}
#| echo: false
N <- 64
simulation <- replicate(
  1000,{
  heads <- rbinom(1, size=N, prob=0.25)
  tails <- N - heads
  steps <- heads - tails
  steps
  }
)
barplot(prop.table(table(simulation)), xlab="Position (0=50-yard line)")
mtext(paste("Observed SD:", round(sd(simulation), 3)))
```

:::

::: {.callout-tip}
## The CLT Revisited

In it's classical form (the form I learned in grad school), the central limit theorem states that for independent samples of $X_i$ with mean $\mu$ and variance $\sigma^2$, the distribution of the sample mean $\bar{X}$ converges with large $n$ to a Normal distribution centered at $\mu$ with predictable variance:

$$
\bar{X} \rightarrow N(\mu, \frac{\sigma^2}{n})
$$
:::

## Selecting Priors

While classical inference is often concerned with selecting the best estimators, Bayesian inference depends heavily on the choice of priors. The latter is not something that's emphasized in most courses (indeed, not in my graduate school classes), and it's usually assumed that priors emerge out of a textbook when needed to solve a Bayesian problem, or that they are extremely vague and uninformative.

What I like about this chapter is its insistence on selecting appropriate priors, and then checking the assumptions reinforced by priors before analyzing any data. This is a sensible thing to do; indeed the choice of different priors may distinguish two anlysts from one another, but to even begin analyzing data one ought to reassure themselves that the prior chosen is at least appropriate.

Consider the example from this chapter, which analyzes heights as Gaussian-distributed. In a way that's surprisingly common in other textbooks, one might suppose that the prior on the mean height $\mu$ is uninformative, for example:

$$
\begin{align}
h_i \sim \text{Normal}(\mu, \sigma^2) \\
\mu \sim \text{Normal}(0, 100^2) \\
\sigma \sim \text{Unif}(0, 50) \\
\end{align}
$$
The choice of prior for $\mu$ is uninformative, yes, but it's actually a preposterous assumption. Human heights are not distributed with such high variance, nor are they centered at zero. But this is exactly the kind of choice that a naive observer would want to make if they knew *nothing* about human heights. Instead the author posits a model that is reasonable given his height of 178cm:

$$
\begin{align}
h_i \sim \text{Normal}(\mu, \sigma^2) \\
\mu \sim \text{Normal}(178, 20^2) \\
\sigma \sim \text{Unif}(0, 50) \\
\end{align}
$$

Below I compare the two choices of priors through a prior predictive check:

:::{.panel-tabset}

### Uninformative

```{r}
#| code-fold: true
sample.mu <- rnorm(10000, mean=0, sd=100)
sample.sigma <- runif(10000, min=0, max=50)
prior.h <- rnorm(10000, mean=sample.mu, sd=sample.sigma)
hist(prior.h, main=NA, xlim=c(-400, 400))
mtext("Prior on Height (cm)")
```
### Informative

```{r}
#| code-fold: true
sample.mu <- rnorm(10000, mean=178, sd=20)
sample.sigma <- runif(10000, min=0, max=50)
prior.h <- rnorm(10000, mean=sample.mu, sd=sample.sigma)
hist(prior.h, main=NA, xlim=c(-400, 400))
mtext("Prior on Height (cm)")
```
:::

The informative prior might seem like "cheating" in some respect because it is more precise to begin with, but such precision is the exact reason we'd like to work with priors in the first place. Priors help to constrain our estimates to ranges we believe plausible. The alternatives--keeping priors vague or reverting to classical methods--might be lauded as "objective" but fail to provide meaningful insight when our sample size is limited (as it almost always is). Even worse, when folks see an objective result that doesn't agree with their intuition, they come up with subjective reasons to discount it, and those reasons are almost never formalized with the same transparency as a prior selection.


```{r}
#| eval: false
#| echo: false
data(Howell1)
adults <- Howell1[Howell1$age>=18, ]
mean.weight <- mean(adults$weight)
fit.heights <- quap(alist(
  height ~ dnorm(mu, sigma),
  mu <- alpha + beta * (weight - mean.weight),
  alpha ~ dnorm(178, 20),
  beta ~ dlnorm(0, 1),
  sigma ~ dunif(0, 50)),
  data=adults)
posterior <- extract.samples(fit.heights, n=10000)
precis(posterior)
# plot posterior inference
weight.seq <- seq(25, 70)
plot(height ~ weight, data=adults, col=rangi2)
mu <- link(fit.heights, data=data.frame(weight=weight.seq))
mu.int89 <- apply(mu, 2, PI, prob=0.89)
lines(weight.seq, apply(mu, 2, mean))
shade(mu.int89, weight.seq)
# plot prediction intervals
sim.heights <- sim(fit.heights, data=list(weight=weight.seq))
heights.int89 <- apply(sim.heights, 2, PI, prob=0.89)
plot(height ~ weight, data=adults, col=rangi2)
lines(weight.seq, apply(mu, 2, mean))
shade(heights.int89, weight.seq)
```
```{r}
#| eval: false
#| echo: false
data(cherry_blossoms)
temps <- cherry_blossoms[complete.cases(cherry_blossoms$temp), ]
num.knots <- 15
knot.locs <- quantile(temps$year, seq(0, 1, length.out=num.knots))
basis <- bs(temps$year, knots=knot.locs[-c(1, num.knots)], degree=3, intercept=T)
fit.temps <- quap(
  alist(
    T ~ dnorm(mu, sigma),
    mu <- alpha + B %*% w,
    alpha ~ dnorm(6, 10),
    w ~ dnorm(0, 1),
    sigma ~ dexp(1)),
  data=list(T=temps$temp, B=basis),
  start=list(w=rep(0, ncol(basis))))
posterior <- extract.samples(fit.temps)
mu <- link(fit.temps)
mu.int97 <- apply(mu, 2, PI, prob=0.97)
plot(temp ~ year, data=cherry_blossoms, col=col.alpha(rangi2, 0.3), pch=16)
shade(mu.int97, temps$year, col=col.alpha("black", 0.5))
```
# Chapter 5 (Multiple Linear Regression)

## Testable Implications

This chapter, which emphasizes multiple regression techniques, begins with a surprising discourse on causal inference. I'm thankful for the opportunity to review some causal concepts, since just like Bayesian methods, causal inference is not often taught in traditional statistics curricula.

Here are some directed acyclic graphs (DAGs) that were proposed, in an attempt to explain the phenomenon of variability in state-level divorce rates:

```{r}
#| label: fig-dags
#| fig-cap: "DAGs from Chapter 5"
#| fig-subcap: 
#|   - "Effect of age (at marriage) on divorce rate mediated by marriage rate."
#|   - "Effect of marriage rate is conditionally independent of divorce rate."
#| layout-ncol: 2
#| code-fold: true
dag.mediator <- dagitty( "dag {
    Age -> Divorce
    Age -> Marriage
    Marriage -> Divorce}")
coordinates(dag.mediator) <- list(
  x=c(Age=0, Divorce=1, Marriage=2),
  y=c(Age=0, Divorce=1, Marriage=0))
plot(dag.mediator)

dag.independ <- dagitty( "dag {
    Age -> Divorce
    Age -> Marriage}")
coordinates(dag.independ) <- list(
  x=c(Age=0, Divorce=1, Marriage=2),
  y=c(Age=0, Divorce=1, Marriage=0))
plot(dag.independ)
```

Now the testable implication of @fig-dags is whether the marriage rate is *conditionally independent* of divorce. In formal terms:

$$
\begin{align}
M \perp D |A \\
\quad \text{where} \quad M:\text{Marriage rate,} \quad D:\text{Divorce rate,} \\
\quad \text{and} \quad A:\text{Age at marriage (median)}
\end{align}
$$
This example introduced new intuition that I had not previously encountered in my graduate statistics training. One, the idea of conditional independence is not new, but knowing that it could be gleaned from a DAG is a valuable lesson. It highlights the value in drawing these diagrams before modeling. Two, the concept of mediators is not new either, but knowing that a mediator model implies all three variables are associated with one another is a good reminder of the differences between data structures and causal implications. We might observe many correlations within an observed set of data, some of them spurious. However, having a DAG to return to makes it easier to theorize how such correlations came to be.

Here's a simulated example of conditional independence, in which marriage rate is conditionally independent of divorce rate, yet a linear regression still picks up on the strong (unadjusted) relationship between the two:

```{r}
#| code-fold: true
set.seed(1818)
N <- 50
age <- rnorm(N)
mar.rate <- rnorm(N, -age)
div.rate <- rnorm(N, age)
precis(lm(div.rate ~ mar.rate), prob=0.95)
```

Without adjusting for age, we would mistakenly conclude that marriage rate is the solely important predictor of divorce rate (with a confidence interval that includes negative boundaries). In reality, this association disappears when we "control" for age:

[**Terminology:** The book makes a compelling argument against use of the word "control", which is common in statistical parlance, because it suggests more power than is truly available. In lieu of an experimental *control*, the best we can do is *adjust* for predictors that might share a causal relationship with the outcome. That is what's accomplished in multiple regression.]{.aside}

```{r}
#| code-fold: true
precis(lm(div.rate ~ mar.rate + age), prob=0.95)
```

## Counterfactual Plots

Of the various diagnostics for multiple regression, I found the proposition of a counterfactual plot most compelling. They are constructed as follows:

1. Choose the predictor (intervention) to manipulate.
2. Define the range of values for the chosen predictor.
3. Leverage the posterior to simulate values of other variables, including both the outcome *and* dependent predictors.

It's the third step that's new for me. I am not used to considering the intermediate effects of a change in the intervention, but that seems to be exactly what Bayesian models are equipped to handle. Here's the textbook example:

```{r}
#| code-fold: true

data("WaffleDivorce")
states <- list(
  A = standardize(WaffleDivorce$MedianAgeMarriage),
  M = standardize(WaffleDivorce$Marriage),
  D = standardize(WaffleDivorce$Divorce))
fit.counterfac <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- alpha + beta_mar * M + beta_age * A,
    alpha ~ dnorm(0, 0.2),
    beta_mar ~ dnorm(0, 0.5),
    beta_age ~ dnorm(0, 0.5),
    sigma ~ dexp(1),
    M ~ dnorm(theta, gamma), ### I'm using theta, gamma for the mediator model
    theta <- eta + delta_age * A,
    eta ~ dnorm(0, 0.2),
    delta_age ~ dnorm(0, 0.5),
    gamma ~ dexp(1)),
  data=states)
precis(fit.counterfac)
```

From this model, it's clear that age and marriage rate are associated through the posterior summary of $\delta_{\text{Age}}$, which is consistently negative. Therefore the effect of an increase in the median age of marriage is associated with a decrease in the marriage rate, which makes perfect sense.

Next I carry out the three steps necessary to create a counterfactual portrait of a change in the median age at marriage, all in the following code block:

```{r}
age.seq <- seq(-2, 2, length.out=10)
post <- extract.samples(fit.counterfac)
sim.counterfac <- list()
# simulate a 1000 x 10 matrix of marriage rates...
sim.counterfac$M <- sapply(
  age.seq,
  function(age) rnorm(1000, post$eta + post$delta_age * age, post$gamma))
# next, simulate a 1000 x 10 matrix of divorce rates, conditional on prev. sim
sim.counterfac$D <- sapply(
  1:length(age.seq),
  function(i) rnorm(
    1000,
    post$alpha + post$beta_age * age.seq[i] + post$beta_mar * sim.counterfac$M[, i],
    post$gamma))
```

I think the trick here is to use the simulated values of the marriage rate (conditional on chosen ages) to simulate--in-turn--values of the divorce rate. I can say confidently that I would not have though to do this, especially within the scope of one model. But the posterior makes this trivial to examine:

```{r}
#| label: fig-counterfactual
#| fig-cap: "Counterfactual Plots from Chapter 5"
#| fig-subcap: 
#|   - "Effect of age (at marriage) on divorce rate, the outcome."
#|   - "Effect of age (at marriage) on marriage rate, the mediator."
#| layout-ncol: 2
#| code-fold: true
plot(age.seq, colMeans(sim.counterfac$D), ylim=c(-2, 2), type="l",
     xlab="Manipulated Age", ylab="Counterfactual Divorce")
shade(apply(sim.counterfac$D, 2, PI), age.seq)

plot(age.seq, colMeans(sim.counterfac$M), ylim=c(-2, 2), type="l",
     xlab="Manipulated Age", ylab="Counterfactual Marriage")
shade(apply(sim.counterfac$M, 2, PI), age.seq)
```

The plot in @fig-counterfactual illustrates the entire strength of associations illustrated earlier in @fig-dags; namely, that age is associated with both marriage rate and divorce rate. Thus, intervening to alter the age of marriage will cascade downstream to effect both the mediator and the outcome, where the outcome is *doubly* affected by both the intervention and the mediator.

[^1]: Statistical Rethinking by Richard McElreath. Second Edition. <https://xcelab.net/rm/>.
[^2]: Efficiency Property of Maximum Likelihood Estimators. <https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Efficiency>.
[^3]: Rethinking package for R. <https://github.com/rmcelreath/rethinking>.
[^4]: Metropolis-Hastings Algorithm. <https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm>.
